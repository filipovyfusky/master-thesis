\chapter{Results}

The segmentation networks introduced in the previous chapter, SegNet, Bayesian SegNet and their simplified versions (Basic) were trained using the described techniques and various hyperparameters. The default optimizer choice for training CNNs is usually Adam, but its implementation in Caffe takes much more memory than other algorithms. That is why the algorithm chosen for training was AdaDelta, which is used by many users of the SegNet architecture \cite{aizawan_github}.

As AdaDelta adapts the learning rate over the course of training, there is no longer a need to manually tune the learning rate decay scheme (which would become another hyperparameter). Therefore, the first hyperparameter tuned was the base learning rate for the AdaDelta algorithm. The search was initiated within a coarse interval of values: $ <10^{-3}, 10^{0}>  $. Since the Caffe implementation of SegNet comes with custom scripts for calculating batch normalisation statistics for the inference phase, checking the validation loss periodically becomes extremely memory demanding and time inefficient. Therefore, the validation loss was checked only once at the end of the last training epoch to ensure that the values of losses had not diverged.  

All variants of SegNet were trained using transfer learning where the encoder weights are pre-trained and either stay unchanged or their learning rate is decreased. In case of Bayesian SegNet and SegNet, the encoder was initiated using \href{http://www.robots.ox.ac.uk/~vgg/research/very_deep/}{VGG16} weights. For SegNet Basic and Bayesian SegNet Basic, the encoder was initiated from a model trained on the CamVid dataset which is available at \href{https://github.com/alexgkendall/SegNet-Tutorial/blob/master/Example_Models/segnet_model_zoo.md}{SegNet Model Zoo}. 

After a reasonable learning rate value was found, the random search was limited to the close interval around it. Then, the training was executed until no further change in the loss function was observed. In the original paper \cite{segnet}, the authors use L2 regularization. The value of the corresponding \textit{weight\_decay} hyperparameter was left unchanged and remained as the SegNet authors suggest.  

The difference observed across the network variants was the time it took to achieve low loss values. This is influenced by the size of the network (Basic versions train faster) and the dropout settings (dropout slows down the training).

Figures \ref{tuning1} and \ref{tuning2} are examples of tuning of the learning rate (Bayesian SegNet). The network was trained using transfer learning. The figures below show two training schemes applied to the pre-trained encoder: in Figure \ref{tuning1}, the encoder weights stay unchanged during the training. This apparently makes it harder for the decoder to adapt. Also, training with learning rates that initially seem to work well makes the loss diverge from the optimal value in the last few epochs. In Figure \ref{tuning2} on the other hand, the encoder weights are allowed to change but only with a decreased learning rate. This scheme tends to give more stable training results and speeds up the training. Therefore, this second scheme was applied to all variants of SegNet.  

It turns out that larger values of learning rate tend to work better with AdaDelta and lead to better values of the loss function. 

\newpage

\begin{figure}[h]
	\begin{center}
		\includegraphics*[width=14cm, keepaspectratio]{obr/bayes_full_rough.png}
	\end{center}
	\vspace{5mm}
	\caption{Coarse search of the \textit{learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the corresponding layers stayed \textbf{unchanged} during the training.} 
	\label{tuning1}
\end{figure}

\vspace{5mm}
\begin{figure}[h]
	\begin{center}
		\includegraphics*[width=14cm, keepaspectratio]{obr/bayes_full_rough_2.png}
	\end{center}
	\vspace{5mm}
	\caption{Coarse search of the \textit{learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the learning rate of the corresponding layers is \textbf{decreased} by the factor of 10 during the training.} 
	\label{tuning2}
\end{figure}

Table \ref{tabulka} summarizes the best training results obtained for all SegNet architectures. The metrics used for evaluation is IOU for each class: class 0 (\textit{background}), class 1 (\textit{path}). It also contains other useful information, such as the inference and training times. In terms of computational cost, it is evident that the best-performing architecture is SegNet Basic. Bayesian versions of SegNet repeat the inference based on the number of MCDO samples and hence take longer to evaluate. The inference runs on GPU as well as the training.

\renewcommand{\arraystretch}{1.1}
%\newcolumntype{C}{>{\centering\arraybackslash}p{3em}}
\begin{table}[h]
	\centering	
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\thead{Architecture} & \thead{base\_lr} & \thead{weight \\ decay} & \thead{batch \\ size} & \thead{MCDO \\ samples}\\		
		\hline
		SegNet & 0.95 & 0.0005 & 4 & - \\
		\hline
		\makecell{SegNet \\ Basic} & 0.75 & 0.0005 & 4 & - \\
		\hline
		\makecell{Bayesian \\ SegNet} & 0.5 & 0.0005 & 4 & 8 \\	
		\hline
		\makecell{Bayesian \\ SegNet Basic} & 0.85 & 0.0005 & 4 & 8 \\
		\hline
		& \thead{IOU \\ class 0} & \thead{IOU \\ class 1} & \thead{Inference \\ time [ms] } & \thead{Training \\ epoch time [s] }\\		
		\hline	
		SegNet & 0.965 & 0.971 & 42 & 368 \\	
		\hline	
		\makecell{SegNet \\ Basic} & 0.966 & 0.972 & 23 & 312 \\	
		\hline	
		\makecell{Bayesian \\ SegNet} & 0.974 & 0.979 & 305 & 432 \\	
		\hline	
		\makecell{Bayesian \\ SegNet Basic} & 0.967 & 0.972 & 177 & 313 \\
		\hline
	\end{tabular}
	\vspace{10mm}
	\caption{Statistics for all SegNet variants on the \textit{test} dataset. The inference was ran on GPU.} 
	\label{tabulka}
\end{table}

Figure \ref{comparison} shows the final segmentation results for several image scenes from the \textit{test} dataset. For Bayesian versions of SegNet, the segmentation comes with the uncertainty plot where light regions mean larger variance of MCDO samples during inference. The uncertainty is averaged over all segmentation classes. We see that the network is more uncertain in the regions that are close to the object boundaries. Also, the full versions of the architectures, SegNet and Bayesian SegNet tend to give more precise results on the boundaries. They are primarily designed for more complex scenes with multiple classes and the encoder is more capable of extracting finer features as the model capacity is higher. In addition, the pre-trained encoder for the full versions was trained on more images compared to the one used for the initialization of the Basic versions. On the other hand, the Basic versions might offer much better performance for practical applications where the number of classes is small.

\begin{figure}[h]
	\begin{center}
		\includegraphics*[width=16cm, keepaspectratio]{obr/result.png}
	\end{center}
	\vspace{5mm}
	\caption{Comparison of the segmentation performance of all SegNet variants. The Bayesian versions of the architecture give the estimate of the model uncertainty, where the lighter regions mean higher variance across the MCDO samples taken during inference.} 
	\label{comparison}
\end{figure}
