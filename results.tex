\chapter{Results}

The segmentation networks introduced in the previous chapter, SegNet, Bayesian SegNet and their simpler versions (Basic) were trained using the described techniques and various hyperparameters. The solver used for optimization was AdaDelta in all cases.
The default choice for CNN is usually Adam, but its implementation in Caffe takes much more memory than other algorithms. That is why the algorithm chosen for training is AdaDelta, which is used by many SegNet users [github implementace segnetu v tensorflow].

As AdaDelta adapts the learning rate over the course of training, there's no longer a need to manually tune the learning rate decay scheme (which would be another hyperparameter). Therefore, the first hyperparameter to tune was the base learning rate. The search was initiated within a coarse interval of values: $ <10^{-3}, 10^{0}>  $. This interval was divided into three sub-regions to make the random choice more balanced. Then, the values within each interval were selected using Uniform random distribution. The training loss was observed for five epochs. Since the Caffe implementation of SegNet comes with custom scripts for calculating batch-normalisation statistics for inference, checking the validation loss periodically becomes extremely memory demanding and time inefficient. Therefore, the validation loss was computed only once at the end of the last training epoch to ensure that the values of losses had not diverged.  

When a reasonable learning rate value range was found, the random search was limited to this interval and the training was executed until no further change in the loss function was observed. 

In the original paper, authors use L2 regularization. This values stayed unchanged and remained the same as the SegNet authors suggest.  

This scheme was applied to all SegNet variants. The difference observed is the time it takes to achieve low loss values. This is influenced by the size of the network (Basic versions train faster) and the dropout settings (dropout slows down the training).  


%https://stackoverflow.com/questions/50853538/caffe-why-dropout-layer-exists-also-in-deploy-testing

ODECISTP PER CHANNEL MEAN PRO INFERENCI, a taky pro test file, nezapomenout oddelat shuffle a mirror!!!

bayesian takes longer to train

TRAINING STRATEGIES

TRANSFER LEARNING + BATCH NORMALIZATION

--------------TRAINING CAFFE NOTES---------------


We train the model with dropout and sample the posterior distribution over the weights at test time using dropout
to obtain the posterior distribution of softmax class probabilities. We take the mean of these samples for our segmentation prediction and use the variance to output model
uncertainty for each class. We take the mean of the per class
variance measurements as an overall measure of model uncertainty. We also explored using the variation ratio as
a measure of uncertainty (i.e. the percentage of samples
which agree with the class prediction) however we found
this to qualitatively produce a more binary measure of
model uncertainty. Fig. 2 shows a schematic of the segmentation prediction and model uncertainty estimate process.