\chapter{Results}

The segmentation networks introduced in the previous chapter, SegNet, Bayesian SegNet and their simpler versions (Basic) were trained using the described techniques and various hyperparameters. The default optimizer choice for training CNN is usually Adam, but its implementation in Caffe takes much more memory than other algorithms. That is why the algorithm chosen for training was AdaDelta, which is used by many SegNet users \cite{aizawan_github}.

As AdaDelta adapts the learning rate over the course of training, there's no longer a need to manually tune the learning rate decay scheme (which would become another hyperparameter). Therefore, the first hyperparameter tuned was the base learning rate for the AdaDelta algorithm. The search was initiated within a coarse interval of values: $ <10^{-3}, 10^{0}>  $. Since the Caffe implementation of SegNet comes with custom scripts for calculating batch normalisation statistics for the inference phase, checking the validation loss periodically becomes extremely memory demanding and time inefficient. Therefore, the validation loss was computed only once at the end of the last training epoch to ensure that the values of losses had not diverged.  

All network variants were trained using transfer learning where the encoder weights are pre-trained and either stay unchanged or their learning rate is decreased. In case of Bayesian SegNet and SegNet, the encoder was initiated using VGG16 weights from [source]. For SegNet Basic and Bayesian SegNet Basic, the encoder was initiated from a model trained on the CamVid dataset [source]. 

When a reasonable learning rate value was found, the random search was limited to the close interval around it. Then, the training was executed until no further change in the loss function was observed. In the original paper \cite{segnet}, the authors use L2 regularization. The value of the corresponding hyperparameter was left unchanged and remained as the SegNet authors suggest.  

The difference observed across the network variants was the time it takes to achieve low loss values. This is influenced by the size of the network (Basic versions train faster) and the dropout settings (dropout slows down the training).

Figure \ref{tuning1} shows an example of tuning of the learning rate (Bayesian SegNet). The network was trained using transfer learning. The figures below show the two procedures applied to the pre-trained encoder: in Figure XY, the encoder weights stay unchanged during the training. This apparently makes it harder for the decoder to adapt. Also, the training with learning rates that seem to be optimal makes the loss diverge from the optimal value in the last few epochs. In Figure XY on the other hand, the encoder weights are allowed to change but only with a decreased learning rate. This scheme tends to give more stable training results and speeds up the training. Therefore, this second scheme was applied to all network variants.  

It turned out that larger values of learning rate tend to lead to better values of the loss function and accuracy on the \textit{test} set. 

\newpage

\begin{figure}[h]
	\begin{center}
		\includegraphics*[width=14cm, keepaspectratio]{obr/bayes_full_rough.png}
	\end{center}
	\vspace{5mm}
	\caption{Coarse search of the \textit{learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the corresponding layers stayed \textbf{unchanged} during the training.} 
	\label{tuning1}
\end{figure}

\vspace{5mm}
\begin{figure}[h]
	\begin{center}
		\includegraphics*[width=14cm, keepaspectratio]{obr/bayes_full_rough_2.png}
	\end{center}
	\vspace{5mm}
	\caption{Coarse search of the \textit{learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the learning rate of the corresponding layers is \textbf{decreased} by the factor of 10 during the training.} 
	\label{}
\end{figure}

Table XY summarized the best training results obtained for all SegNet architectures. The metrics used for evaluation is IOU for each class: class 0 (background), class 1 (path). For Bayesian versions of SegNet, the segmentation comes with the uncertainty plot where the darker regions mean larger 

\renewcommand{\arraystretch}{1.1}
%\newcolumntype{C}{>{\centering\arraybackslash}p{3em}}
\begin{figure}[t]
\centering	
	\begin{tabular}{|c|c|c|c|c|}
	\hline
	\thead{Architecture} & \thead{base\_lr} & \thead{weight \\ decay} & \thead{batch \\ size} & \thead{MCDO \\ samples}\\		
	\hline
	SegNet & 0.95 & 0.0005 & 4 & - \\
	\hline
	\makecell{SegNet \\ Basic} & 0.75 & 0.0005 & 4 & - \\
	\hline
	\makecell{Bayesian \\ SegNet} & 0.5 & 0.0005 & 4 & 8 \\	
	\hline
	\makecell{Bayesian \\ SegNet Basic} & 0.85 & 0.0005 & 4 & 8 \\
	\hline
	& \thead{IOU \\ class 0} & \thead{IOU \\ class 1} & \thead{Inference \\ time [ms] } & \thead{Training \\ epoch [ms] }\\		
	\hline	
	SegNet & 0.965 & 0.971 & 42 & zdar \\	
	\hline	
	\makecell{SegNet \\ Basic} & 0.966 & 0.972 & 23 & zdar \\	
	\hline	
	\makecell{Bayesian \\ SegNet} & 0.974 & 0.979 & 305 & zdar \\	
	\hline	
	\makecell{Bayesian \\ SegNet Basic} & 0.967 & 0.972 & 177 & zdar \\
	\hline
	\end{tabular}
\vspace{10mm}
\caption{Coarse search of the \textit{learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the corresponding layers stayed \textbf{unchanged} during the training.} 
\label{zdarec}
\end{figure}

etgegergergggggggggggggggggergergegegegergergergergerge ergwerg er e erge rg erg erg erge e erge rg . eg eeg erger 
 egergerg egerge 
  egegergege
  rgerg ergergegergergergerg

\begin{figure}[h]
	\begin{center}
		\includegraphics*[width=16cm, keepaspectratio]{obr/result.png}
	\end{center}
	\vspace{5mm}
	\caption{Coarse search of the \textit{learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the learning rate of the corresponding layers is \textbf{decreased} by the factor of 10 during the training.} 
	\label{}
\end{figure}
