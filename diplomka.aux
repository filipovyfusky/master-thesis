\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\catcode `-\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{stanford-L11}
\citation{mwiti}
\citation{sergios}
\citation{sergios}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Segmentation of an urban road scene \cite  {sergios}\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{segment}{{1.1}{3}{Segmentation of an urban road scene \cite {sergios}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Problem statements}{4}{chapter.2}}
\citation{mehlig}
\citation{santiago}
\citation{santiago}
\citation{santiago}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Research and theory}{5}{chapter.3}}
\newlabel{research}{{3}{5}{Research and theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Architecture of artificial neural networks}{5}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Feed-forward networks}{5}{subsection.3.1.1}}
\citation{santiago}
\citation{santiago}
\citation{goodfellow}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}McCulloch-Pitts neurons}{6}{subsection.3.1.2}}
\newlabel{neuron_output}{{3.5}{6}{McCulloch-Pitts neurons}{equation.3.1.5}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{groman}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic diagram of a McCulloch-Pitts neuron. The strength of the connection from neuron $ j $ to neuron $ i $ is denoted by $ w_{ij} $ \cite  {mehlig}\relax }}{7}{figure.caption.3}}
\newlabel{neuron}{{3.1}{7}{Schematic diagram of a McCulloch-Pitts neuron. The strength of the connection from neuron $ j $ to neuron $ i $ is denoted by $ w_{ij} $ \cite {mehlig}\relax }{figure.caption.3}{}}
\newlabel{local_field}{{3.6}{7}{McCulloch-Pitts neurons}{equation.3.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Activation functions}{7}{subsection.3.1.3}}
\citation{groman}
\citation{stanford-github}
\citation{groman}
\citation{stanford-github}
\citation{groman}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{8}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sigmoid function and its derivative. Notice that the derivative goes to zero very quickly.\relax }}{8}{figure.caption.5}}
\citation{stanford-github}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Hyperbolic tangent}{9}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Hyperbolic tangent and its derivative.\relax }}{9}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsubsection}{Rectified linear unit (ReLu)}{9}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces ReLu and its derivative. ReLu does not saturate!\relax }}{9}{figure.caption.9}}
\citation{stanford-L4}
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLu}{10}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Leaky ReLu and its derivative.\relax }}{10}{figure.caption.11}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Multilayer perceptrons}{11}{subsection.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Perceptron with one hidden layer. \cite  {mehlig}\relax }}{11}{figure.caption.12}}
\newlabel{perceptron}{{3.6}{11}{Perceptron with one hidden layer. \cite {mehlig}\relax }{figure.caption.12}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Output classifier - softmax}{12}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Softmax classifier: the neurons in this layer are not independent. \cite  {mehlig}\relax }}{12}{figure.caption.14}}
\newlabel{softmax}{{3.7}{12}{Softmax classifier: the neurons in this layer are not independent. \cite {mehlig}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Linear separability}{12}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Linearly separable (left) and not linearly separable problems (right). The decision boundary needs to be piece-wise linear for the not linearly separable problem \cite  {mehlig}\relax }}{12}{figure.caption.16}}
\newlabel{separability}{{3.8}{12}{Linearly separable (left) and not linearly separable problems (right). The decision boundary needs to be piece-wise linear for the not linearly separable problem \cite {mehlig}\relax }{figure.caption.16}{}}
\citation{notes}
\citation{mehlig}
\citation{groman}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training of artificial neural networks}{14}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Loss function}{14}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Squared Error (MSE)}{14}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Negative Log Likelihood}{14}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{Cross Entropy Loss}{14}{section*.19}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{notes}
\citation{notes}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Gradient optimization and backpropagation}{15}{subsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Backpropagation algorithm: the states of the neurons are updated forward (from left to right) while errors are updated backward (right to left) \cite  {mehlig}\relax }}{15}{figure.caption.20}}
\newlabel{backprop}{{3.9}{15}{Backpropagation algorithm: the states of the neurons are updated forward (from left to right) while errors are updated backward (right to left) \cite {mehlig}\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{15}{section*.21}}
\citation{coors}
\citation{coors}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Effect of the learning rate on optimization: the value must be chosen carefully for the algorithm to converge. \cite  {coors}\relax }}{16}{figure.caption.22}}
\newlabel{learning_rate}{{3.10}{16}{Effect of the learning rate on optimization: the value must be chosen carefully for the algorithm to converge. \cite {coors}\relax }{figure.caption.22}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{eniola}
\citation{mehlig}
\citation{stanford-L4}
\citation{mehlig}
\newlabel{update_rule}{{3.20}{17}{Gradient descent}{equation.3.2.20}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient descent \cite  {mehlig}\relax }}{17}{algorithm.1}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic gradient descent}{17}{section*.23}}
\@writefile{toc}{\contentsline {subsubsection}{Vanishing and exploding gradient problems}{17}{section*.24}}
\citation{stanford-L7}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-github}
\citation{stanford-L7}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Momentum}{18}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Momentum (left) and Nesterov's Momentum (right). \cite  {mehlig}\relax }}{18}{figure.caption.26}}
\newlabel{momentum}{{3.11}{18}{Momentum (left) and Nesterov's Momentum (right). \cite {mehlig}\relax }{figure.caption.26}{}}
\citation{stanford-L7}
\citation{stanford-L7}
\citation{groman}
\citation{bushaev}
\citation{groman}
\citation{groman}
\citation{groman}
\@writefile{toc}{\contentsline {subsubsection}{Other optimization algorithms}{19}{section*.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Comparison of different optimization algorithms. \cite  {groman}\relax }}{19}{figure.caption.28}}
\newlabel{algorithms}{{3.12}{19}{Comparison of different optimization algorithms. \cite {groman}\relax }{figure.caption.28}{}}
\citation{mehlig}
\citation{stanford-L6}
\citation{stanford-L6}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L6}
\citation{mehlig}
\citation{issue}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Improving training performance}{20}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{Initialization of weights and thresholds}{20}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{Overfitting and regularization}{20}{section*.30}}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L7}
\citation{arvi}
\citation{arvi}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalisation}{21}{section*.31}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout}{21}{section*.32}}
\newlabel{dropout_sec}{{3.2.3}{21}{Dropout}{section*.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces ANN without (left) and with dropout (right). \cite  {arvi}\relax }}{21}{figure.caption.33}}
\newlabel{dropout}{{3.13}{21}{ANN without (left) and with dropout (right). \cite {arvi}\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data augmentation}{21}{section*.34}}
\@writefile{toc}{\contentsline {subsubsection}{Early stopping}{21}{section*.35}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L7}
\citation{stanford-L7}
\citation{stanford-github}
\citation{stanford-L4}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Progress of training and validation losses. The plot is schematic, and the data is smoothed. The training is stopped when the validation energy begins to increase. \cite  {mehlig}\relax }}{22}{figure.caption.36}}
\newlabel{}{{3.14}{22}{Progress of training and validation losses. The plot is schematic, and the data is smoothed. The training is stopped when the validation energy begins to increase. \cite {mehlig}\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning}{22}{section*.37}}
\@writefile{toc}{\contentsline {subsubsection}{Data pre-processing}{22}{section*.38}}
\citation{stanford-github}
\citation{stanford-github}
\citation{coors}
\citation{coors}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Convolutional neural networks}{23}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}CNN layer types}{23}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{Convolution layers}{23}{section*.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The full-depth convolution operation in a convolutional layer. The input size corresponds to a small RGB image. The result of the series of convolutions is a tensor of stacked activation maps for the filters used in the layer. \cite  {coors}\relax }}{23}{figure.caption.40}}
\newlabel{conv}{{3.15}{23}{The full-depth convolution operation in a convolutional layer. The input size corresponds to a small RGB image. The result of the series of convolutions is a tensor of stacked activation maps for the filters used in the layer. \cite {coors}\relax }{figure.caption.40}{}}
\citation{mehlig}
\citation{segnet}
\citation{coors}
\citation{coors}
\citation{stanford-github}
\citation{notes}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-github}
\@writefile{toc}{\contentsline {subsubsection}{Pooling layers}{24}{section*.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Max-pooling of size 2x2 and stride 2. \cite  {coors}\relax }}{24}{figure.caption.42}}
\newlabel{pool}{{3.16}{24}{Max-pooling of size 2x2 and stride 2. \cite {coors}\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fully connected layers (FCN)}{24}{section*.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Schematic of the standard CNN topology for image classification. \cite  {mehlig}\relax }}{24}{figure.caption.44}}
\newlabel{pool}{{3.17}{24}{Schematic of the standard CNN topology for image classification. \cite {mehlig}\relax }{figure.caption.44}{}}
\citation{krizhevsky}
\citation{lecun}
\citation{stanford-github}
\citation{szegedy}
\citation{stanford-github}
\citation{vgg}
\citation{stanford-github}
\citation{resnet}
\citation{stanford-github}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Examples of CNN architectures}{25}{subsection.3.3.2}}
\citation{coufal}
\citation{segnet}
\citation{bayesian}
\citation{stanford-L11}
\citation{segnet}
\citation{segnet}
\citation{segnet}
\citation{segnet}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Semantic segmentation}{26}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Encoder-decoder architecture}{26}{subsection.3.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces SegNet - an example of encoder-decoder CNN architecture. \cite  {segnet}\relax }}{26}{figure.caption.45}}
\newlabel{encoder}{{3.18}{26}{SegNet - an example of encoder-decoder CNN architecture. \cite {segnet}\relax }{figure.caption.45}{}}
\citation{stanford-L11}
\citation{theano}
\citation{theano}
\citation{segnet}
\citation{stanford-L11}
\@writefile{toc}{\contentsline {subsubsection}{Input upsampling}{27}{section*.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Transposed convolution. \cite  {theano}\relax }}{27}{figure.caption.47}}
\newlabel{transposed}{{3.19}{27}{Transposed convolution. \cite {theano}\relax }{figure.caption.47}{}}
\citation{segnet_tut}
\citation{segnet_tut}
\citation{segnet}
\citation{segnet}
\citation{bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Max-unpooling. The locations of the maximum elements were saved during max-pooling. The remaining elements are set to zero.\relax }}{28}{figure.caption.48}}
\newlabel{unpool}{{3.20}{28}{Max-unpooling. The locations of the maximum elements were saved during max-pooling. The remaining elements are set to zero.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}SegNet}{28}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{SegNet - encoder}{28}{section*.49}}
\@writefile{toc}{\contentsline {subsubsection}{SegNet - decoder}{28}{section*.50}}
\citation{bayesian}
\citation{bayesian}
\citation{iou}
\citation{iou}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Bayesian SegNet}{29}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{Monte Carlo Dropout}{29}{section*.51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Evaluating segmentation performance}{29}{subsection.3.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Intersection over union. \cite  {iou}\relax }}{29}{figure.caption.52}}
\newlabel{iou}{{3.21}{29}{Intersection over union. \cite {iou}\relax }{figure.caption.52}{}}
\citation{filip_github}
\citation{nvidia}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation and method}{30}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}CPU vs. GPU for training ANN}{30}{section.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor cores}{30}{section*.53}}
\citation{caffe}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Libraries for ANN}{31}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Caffe}{31}{subsection.4.2.1}}
\citation{gigabyte}
\citation{gigabyte}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Setting up environment for Caffe}{32}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Hardware configuration}{32}{subsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces GIGABYTE GeForce RTX 2060 SUPER AORUS 8G. \cite  {gigabyte}\relax }}{32}{figure.caption.54}}
\newlabel{grafika}{{4.1}{32}{GIGABYTE GeForce RTX 2060 SUPER AORUS 8G. \cite {gigabyte}\relax }{figure.caption.54}{}}
\citation{nvidia_dev}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Software configuration}{33}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Operating system}{33}{section*.55}}
\@writefile{toc}{\contentsline {subsubsection}{Enabling NVIDIA driver}{33}{section*.56}}
\@writefile{toc}{\contentsline {subsubsection}{CUDA installation}{33}{section*.57}}
\citation{nvidia_dev}
\citation{nvidia_dev}
\@writefile{toc}{\contentsline {subsubsection}{Installation of cuDNN}{34}{section*.58}}
\citation{caffe}
\@writefile{toc}{\contentsline {subsubsection}{Setting up Python editor}{35}{section*.59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Building Caffe for SegNet}{35}{subsection.4.3.3}}
\citation{filip_github_caffe}
\citation{labelbox}
\citation{filip_github}
\citation{caffe}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Image annotation}{37}{section.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{Labelbox}{37}{section*.60}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Setting up SegNet}{37}{section.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Solver settings}{37}{subsection.4.5.1}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.1}Contents of \textit  {solver.prototxt}}{37}{lstlisting.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Training}{38}{subsection.4.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Input layer and input pre-processing}{38}{section*.61}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.2}Input layer in \textit  {train.prototxt}}{38}{lstlisting.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{Output dimensions}{39}{section*.62}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.3}Setting number of outputs in \textit  {train.prototxt}}{39}{lstlisting.4.3}}
\citation{segnet}
\@writefile{toc}{\contentsline {subsubsection}{Softmax classifier}{40}{section*.63}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.4}Output layers of \textit  {train.prototxt}}{40}{lstlisting.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{Training initialization}{40}{section*.64}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.5}Setting up \textit  {train.prototxt} for transfer learning}{41}{lstlisting.4.5}}
\citation{segnet_get_started}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Inference}{42}{subsection.4.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{Calculating statistics for batch normalisation}{42}{section*.65}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.6}Replacing input layer type in \textit  {inference.prototxt}}{42}{lstlisting.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{Running the Segmentation}{42}{section*.66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Testing}{43}{subsection.4.5.4}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.7}Setting up the input layer of \textit  {test.prototxt}}{43}{lstlisting.4.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Bayesian SegNet}{44}{subsection.4.5.5}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4.8}Setting MCDO in \textit  {inference.prototxt}}{44}{lstlisting.4.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.6}SegNet Basic and Bayesian SegNet Basic}{45}{subsection.4.5.6}}
\citation{stanford-github}
\citation{stanford-github}
\citation{stanford-L7}
\citation{stanford-L6}
\citation{stanford-L6}
\citation{stanford-L6}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Optimization of Hyperparameters}{46}{section.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-validation Strategy}{46}{section*.67}}
\@writefile{toc}{\contentsline {subsubsection}{Optimizer}{46}{section*.68}}
\@writefile{toc}{\contentsline {subsubsection}{Learning Rate}{46}{section*.69}}
\@writefile{toc}{\contentsline {subsubsection}{Regularisation}{46}{section*.70}}
\citation{aizawan_github}
\citation{segnet}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{47}{chapter.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Coarse search of the \textit  {learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the corresponding layers stayed \textbf  {unchanged} during the training.\relax }}{48}{figure.caption.71}}
\newlabel{tuning1}{{5.1}{48}{Coarse search of the \textit {learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the corresponding layers stayed \textbf {unchanged} during the training.\relax }{figure.caption.71}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Coarse search of the \textit  {learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the learning rate of the corresponding layers is \textbf  {decreased} by the factor of 10 during the training.\relax }}{48}{figure.caption.72}}
\newlabel{tuning2}{{5.2}{48}{Coarse search of the \textit {learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the learning rate of the corresponding layers is \textbf {decreased} by the factor of 10 during the training.\relax }{figure.caption.72}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Coarse search of the \textit  {learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the corresponding layers stayed \textbf  {unchanged} during the training.\relax }}{49}{table.caption.73}}
\newlabel{tabulka}{{5.1}{49}{Coarse search of the \textit {learning\_rate} parameter, Bayesian SegNet. The training loss is observed for 30 epochs and the data is smoothed. The encoder was initialized using pre-trained VGG16 model and the corresponding layers stayed \textbf {unchanged} during the training.\relax }{table.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison of the segmentation performance of all SegNet variants. The Bayesian versions of the architecture give the estimate of the model uncertainty, where the lighter regions mean higher variance across the MCDO samples taken during inference.\relax }}{50}{figure.caption.74}}
\newlabel{comparison}{{5.3}{50}{Comparison of the segmentation performance of all SegNet variants. The Bayesian versions of the architecture give the estimate of the model uncertainty, where the lighter regions mean higher variance across the MCDO samples taken during inference.\relax }{figure.caption.74}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion and future work}{51}{chapter.6}}
\bibcite{mwiti}{1}
\bibcite{sergios}{2}
\bibcite{mehlig}{3}
\bibcite{santiago}{4}
\bibcite{goodfellow}{5}
\bibcite{groman}{6}
\bibcite{stanford-github}{7}
\bibcite{stanford-L4}{8}
\bibcite{stanford-L6}{9}
\bibcite{stanford-L7}{10}
\bibcite{stanford-L11}{11}
\bibcite{coors}{12}
\bibcite{eniola}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Bibliography}{52}{chapter.7}}
\bibcite{bushaev}{14}
\bibcite{issue}{15}
\bibcite{arvi}{16}
\bibcite{krizhevsky}{17}
\bibcite{lecun}{18}
\bibcite{szegedy}{19}
\bibcite{vgg}{20}
\bibcite{resnet}{21}
\bibcite{coufal}{22}
\bibcite{bayesian}{23}
\bibcite{segnet}{24}
\bibcite{segnet_tut}{25}
\bibcite{zeltner}{26}
\bibcite{segnet_get_started}{27}
\bibcite{iou}{28}
\bibcite{filip_github}{29}
\bibcite{nvidia}{30}
\bibcite{nvidia_dev}{31}
\bibcite{caffe}{32}
\bibcite{filip_github_caffe}{33}
\bibcite{aizawan_github}{34}
\bibcite{labelbox}{35}
\bibcite{theano}{36}
\bibcite{gigabyte}{37}
\bibcite{notes}{38}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Seznam použitých zkratek a symbolů}{56}{chapter.8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Seznam příloh}{57}{chapter.9}}
\gdef\pocetstran{57}
