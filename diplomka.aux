\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\catcode `-\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{mwiti}
\citation{sergios}
\citation{sergios}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Segmentation of an urban road scene \cite  {sergios}\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{segment}{{1.1}{3}{Segmentation of an urban road scene \cite {sergios}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Problem statements}{4}{chapter.2}}
\citation{mehlig}
\citation{santiago}
\citation{santiago}
\citation{santiago}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Research and theory}{5}{chapter.3}}
\newlabel{research}{{3}{5}{Research and theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Architecture of artificial neural networks}{5}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Feed-forward networks}{5}{subsection.3.1.1}}
\citation{santiago}
\citation{goodfellow}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}McCulloch-Pitts neurons}{6}{subsection.3.1.2}}
\newlabel{neuron_output}{{3.5}{6}{McCulloch-Pitts neurons}{equation.3.1.5}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic diagram of a McCulloch-Pitts neuron. The strength of the connection from neuron $ j $ to neuron $ i $ is denoted by $ w_{ij} $ \cite  {mehlig}\relax }}{7}{figure.caption.3}}
\newlabel{neuron}{{3.1}{7}{Schematic diagram of a McCulloch-Pitts neuron. The strength of the connection from neuron $ j $ to neuron $ i $ is denoted by $ w_{ij} $ \cite {mehlig}\relax }{figure.caption.3}{}}
\newlabel{local_field}{{3.6}{7}{McCulloch-Pitts neurons}{equation.3.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Activation functions}{7}{subsection.3.1.3}}
\citation{groman}
\citation{stanford-github}
\citation{stanford-github}
\citation{groman}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{8}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sigmoid function and its derivative. Notice that the derivative goes to zero very soon.\relax }}{8}{figure.caption.5}}
\citation{stanford-github}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Hyperbolic tangent}{9}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Hyperbolic tangent and its derivative.\relax }}{9}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsubsection}{Rectified linear unit (ReLu)}{9}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces ReLu and its derivative. ReLu does not saturate!\relax }}{9}{figure.caption.9}}
\citation{stanford-L4}
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLu}{10}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Leaky ReLu and its derivative.\relax }}{10}{figure.caption.11}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Multilayer perceptrons}{11}{subsection.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Perceptron with one hidden layer \cite  {mehlig}\relax }}{11}{figure.caption.12}}
\newlabel{perceptron}{{3.6}{11}{Perceptron with one hidden layer \cite {mehlig}\relax }{figure.caption.12}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Output classifier - softmax}{12}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Softmax classifier: the neurons in this layer are not independent \cite  {mehlig}\relax }}{12}{figure.caption.14}}
\newlabel{softmax}{{3.7}{12}{Softmax classifier: the neurons in this layer are not independent \cite {mehlig}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Linear separability}{12}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Linearly separable (left) and not linearly separable problems (right). The decision boundary needs to be piece-wise linear for the not linearly separable problem \cite  {mehlig}\relax }}{12}{figure.caption.16}}
\newlabel{separability}{{3.8}{12}{Linearly separable (left) and not linearly separable problems (right). The decision boundary needs to be piece-wise linear for the not linearly separable problem \cite {mehlig}\relax }{figure.caption.16}{}}
\citation{groman}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training of artificial neural networks}{14}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Loss function}{14}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Squared Error (MSE)}{14}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Negative Log Likelihood}{14}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{Cross Entropy Loss}{14}{section*.19}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Gradient optimization and backpropagation}{15}{subsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Backpropagation algorithm: the states of the neurons are updated forward (from left to right) while errors are updated backward (right to left) \cite  {mehlig}\relax }}{15}{figure.caption.20}}
\newlabel{backprop}{{3.9}{15}{Backpropagation algorithm: the states of the neurons are updated forward (from left to right) while errors are updated backward (right to left) \cite {mehlig}\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{15}{section*.21}}
\citation{coors}
\citation{coors}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Effect of the learning rate on optimization: the value must be chosen carefully for the algorithm to converge \cite  {coors}\relax }}{16}{figure.caption.22}}
\newlabel{learning_rate}{{3.10}{16}{Effect of the learning rate on optimization: the value must be chosen carefully for the algorithm to converge \cite {coors}\relax }{figure.caption.22}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{eniola}
\citation{mehlig}
\citation{stanford-L4}
\citation{mehlig}
\newlabel{update_rule}{{3.20}{17}{Gradient descent}{equation.3.2.20}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient descent\relax }}{17}{algorithm.1}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic gradient descent}{17}{section*.23}}
\@writefile{toc}{\contentsline {subsubsection}{Vanishing and exploding gradient problems}{17}{section*.24}}
\citation{stanford-L7}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-github}
\citation{stanford-L7}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Momentum}{18}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Momentum (left) and Nesterov's Momentum (right) \cite  {mehlig}\relax }}{18}{figure.caption.26}}
\newlabel{momentum}{{3.11}{18}{Momentum (left) and Nesterov's Momentum (right) \cite {mehlig}\relax }{figure.caption.26}{}}
\citation{stanford-L7}
\citation{stanford-L7}
\citation{groman}
\citation{bushaev}
\citation{groman}
\citation{groman}
\citation{groman}
\@writefile{toc}{\contentsline {subsubsection}{Other optimization algorithms}{19}{section*.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Comparison of different optimization algorithms \cite  {groman}\relax }}{19}{figure.caption.28}}
\newlabel{algorithms}{{3.12}{19}{Comparison of different optimization algorithms \cite {groman}\relax }{figure.caption.28}{}}
\citation{mehlig}
\citation{stanford-L6}
\citation{stanford-L6}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L6}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Improving training performance}{20}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{Initialization of weights and thresholds}{20}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{Overfitting and regularization}{20}{section*.30}}
\citation{issue}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L7}
\citation{arvi}
\citation{arvi}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalisation}{21}{section*.31}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout}{21}{section*.32}}
\newlabel{dropout_sec}{{3.2.3}{21}{Dropout}{section*.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces ANN without (left) and with dropout (right) \cite  {arvi}\relax }}{21}{figure.caption.33}}
\newlabel{dropout}{{3.13}{21}{ANN without (left) and with dropout (right) \cite {arvi}\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data augmentation}{21}{section*.34}}
\@writefile{toc}{\contentsline {subsubsection}{Early stopping}{21}{section*.35}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L7}
\citation{stanford-L7}
\citation{stanford-github}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Progress of training and validation losses. The plot is schematic, and the data is smoothed. The training is stopped when the validation energy begins to increase \cite  {mehlig}\relax }}{22}{figure.caption.36}}
\newlabel{dropout}{{3.14}{22}{Progress of training and validation losses. The plot is schematic, and the data is smoothed. The training is stopped when the validation energy begins to increase \cite {mehlig}\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning}{22}{section*.37}}
\@writefile{toc}{\contentsline {subsubsection}{Data pre-processing}{22}{section*.38}}
\citation{stanford-github}
\citation{stanford-github}
\citation{coors}
\citation{coors}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Convolutional neural networks}{23}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}CNN layer types}{23}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{Convolution layers}{23}{section*.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The full-depth convolution operation in a convolutional layer. The input size corresponds to a small RGB image. The result of the series of convolutions is a tensor of stacked activation maps for the filters used in the layer. \cite  {coors}\relax }}{23}{figure.caption.40}}
\newlabel{conv}{{3.15}{23}{The full-depth convolution operation in a convolutional layer. The input size corresponds to a small RGB image. The result of the series of convolutions is a tensor of stacked activation maps for the filters used in the layer. \cite {coors}\relax }{figure.caption.40}{}}
\citation{mehlig}
\citation{coors}
\citation{coors}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Pooling layers}{24}{section*.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Max-pooling of size 2x2 and stride 2. \cite  {coors}\relax }}{24}{figure.caption.42}}
\newlabel{pool}{{3.16}{24}{Max-pooling of size 2x2 and stride 2. \cite {coors}\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fully connected layers (FCN)}{24}{section*.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Schematic of the standard CNN topology for image classification. \cite  {mehlig}\relax }}{24}{figure.caption.44}}
\newlabel{pool}{{3.17}{24}{Schematic of the standard CNN topology for image classification. \cite {mehlig}\relax }{figure.caption.44}{}}
\citation{stanford-github}
\citation{krizhevsky}
\citation{lecun}
\citation{stanford-github}
\citation{szegedy}
\citation{stanford-github}
\citation{vgg}
\citation{stanford-github}
\citation{resnet}
\citation{stanford-github}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Examples of CNN architectures}{25}{subsection.3.3.2}}
\citation{coufal}
\citation{segnet}
\citation{bayesian}
\citation{zeltner}
\citation{zeltner}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Semantic segmentation}{26}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Encoder-decoder architecture}{26}{subsection.3.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Example of encoder-decoder CNN architecture. \cite  {zeltner}\relax }}{26}{figure.caption.45}}
\newlabel{encoder}{{3.18}{26}{Example of encoder-decoder CNN architecture. \cite {zeltner}\relax }{figure.caption.45}{}}
\citation{segnet}
\citation{segnet}
\citation{segnet}
\citation{segnet_tut}
\@writefile{toc}{\contentsline {subsubsection}{Input upsampling}{27}{section*.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Transposed convolution. ZDROJ THEANO\relax }}{27}{figure.caption.47}}
\newlabel{transposed}{{3.19}{27}{Transposed convolution. ZDROJ THEANO\relax }{figure.caption.47}{}}
\citation{segnet_tut}
\citation{segnet}
\citation{segnet}
\citation{bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Max-unpooling. The locations of the maximum elements were saved during max-pooling. The remaining elements are set to zero.\relax }}{28}{figure.caption.48}}
\newlabel{unpool}{{3.20}{28}{Max-unpooling. The locations of the maximum elements were saved during max-pooling. The remaining elements are set to zero.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}SegNet}{28}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{SegNet - encoder}{28}{section*.49}}
\@writefile{toc}{\contentsline {subsubsection}{SegNet - decoder}{28}{section*.50}}
\citation{bayesian}
\citation{bayesian}
\citation{iou}
\citation{iou}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Bayesian SegNet}{29}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{Monte Carlo Dropout}{29}{section*.51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Evaluating segmentation performance}{29}{subsection.3.4.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Intersection over union. \cite  {iou}\relax }}{29}{figure.caption.52}}
\newlabel{iou}{{3.21}{29}{Intersection over union. \cite {iou}\relax }{figure.caption.52}{}}
\citation{filip_github}
\citation{nvidia}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation and method}{30}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}CPU vs. GPU for training ANN}{30}{section.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor cores}{30}{section*.53}}
\citation{caffe}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Libraries for ANN}{31}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Caffe}{31}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Setting up environment for Caffe}{31}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Hardware configuration}{31}{subsection.4.3.1}}
\citation{nvidia_dev}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Software configuration}{32}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Operating system}{32}{section*.54}}
\@writefile{toc}{\contentsline {subsubsection}{Enabling NVIDIA driver}{32}{section*.55}}
\@writefile{toc}{\contentsline {subsubsection}{CUDA installation}{32}{section*.56}}
\citation{nvidia_dev}
\citation{nvidia_dev}
\@writefile{toc}{\contentsline {subsubsection}{Installation of cuDNN}{33}{section*.57}}
\citation{caffe}
\@writefile{toc}{\contentsline {subsubsection}{Setting up Python editor}{34}{section*.58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Building Caffe for SegNet}{34}{subsection.4.3.3}}
\citation{filip_github_caffe}
\citation{labelbox}
\citation{filip_github}
\citation{caffe}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Image annotation}{36}{section.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{Labelbox}{36}{section*.59}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Setting up SegNet}{36}{section.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Solver settings}{36}{subsection.4.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Training}{37}{subsection.4.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Input layer and input pre-processing}{37}{section*.60}}
\citation{segnet}
\@writefile{toc}{\contentsline {subsubsection}{Output dimensions}{38}{section*.61}}
\@writefile{toc}{\contentsline {subsubsection}{Softmax classifier}{38}{section*.62}}
\@writefile{toc}{\contentsline {subsubsection}{Training initialization}{39}{section*.63}}
\citation{segnet_get_started}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Inference}{40}{subsection.4.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{Calculating statistics for batch normalisation}{40}{section*.64}}
\@writefile{toc}{\contentsline {subsubsection}{Running the Segmentation}{41}{section*.65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Testing}{41}{subsection.4.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Bayesian SegNet}{42}{subsection.4.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.6}SegNet Basic and Bayesian SegNet Basic}{43}{subsection.4.5.6}}
\citation{stanford-github}
\citation{stanford-github}
\citation{stanford-L7}
\citation{stanford-L6}
\citation{stanford-L6}
\citation{stanford-L6}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Optimization of Hyperparameters}{44}{section.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-validation Strategy}{44}{section*.66}}
\@writefile{toc}{\contentsline {subsubsection}{Optimizer}{44}{section*.67}}
\@writefile{toc}{\contentsline {subsubsection}{Learning Rate}{44}{section*.68}}
\@writefile{toc}{\contentsline {subsubsection}{Regularisation}{44}{section*.69}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{45}{chapter.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Discussion and Future Work}{46}{chapter.6}}
\bibcite{mwiti}{1}
\bibcite{sergios}{2}
\bibcite{mehlig}{3}
\bibcite{santiago}{4}
\bibcite{goodfellow}{5}
\bibcite{groman}{6}
\bibcite{stanford-github}{7}
\bibcite{stanford-L4}{8}
\bibcite{stanford-L6}{9}
\bibcite{stanford-L7}{10}
\bibcite{coors}{11}
\bibcite{eniola}{12}
\bibcite{bushaev}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Bibliography}{47}{chapter.7}}
\bibcite{issue}{14}
\bibcite{arvi}{15}
\bibcite{krizhevsky}{16}
\bibcite{lecun}{17}
\bibcite{szegedy}{18}
\bibcite{vgg}{19}
\bibcite{resnet}{20}
\bibcite{coufal}{21}
\bibcite{bayesian}{22}
\bibcite{segnet}{23}
\bibcite{segnet_tut}{24}
\bibcite{zeltner}{25}
\bibcite{segnet_get_started}{26}
\bibcite{iou}{27}
\bibcite{filip_github}{28}
\bibcite{nvidia}{29}
\bibcite{nvidia_dev}{30}
\bibcite{caffe}{31}
\bibcite{filip_github_caffe}{32}
\bibcite{labelbox}{33}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Seznam použitých zkratek a symbolů}{50}{chapter.8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Seznam příloh}{51}{chapter.9}}
\gdef\pocetstran{51}
