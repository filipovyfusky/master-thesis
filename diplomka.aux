\relax 
\providecommand\hyper@newdestlabel[2]{}
\catcode `"\active 
\catcode `-\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\citation{mwiti}
\citation{sergios}
\citation{sergios}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Segmentation of an urban road scene \cite  {sergios}\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{segment}{{1.1}{3}{Segmentation of an urban road scene \cite {sergios}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Problem statements}{4}{chapter.2}}
\citation{mehlig}
\citation{santiago}
\citation{santiago}
\citation{santiago}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Research and theory}{5}{chapter.3}}
\newlabel{research}{{3}{5}{Research and theory}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Architecture of artificial neural networks}{5}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Feed-forward networks}{5}{subsection.3.1.1}}
\citation{santiago}
\citation{goodfellow}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}McCulloch-Pitts neurons}{6}{subsection.3.1.2}}
\newlabel{neuron_output}{{3.5}{6}{McCulloch-Pitts neurons}{equation.3.1.5}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Schematic diagram of a McCulloch-Pitts neuron. The strength of the connection from neuron $ j $ to neuron $ i $ is denoted by $ w_{ij} $ \cite  {mehlig}\relax }}{7}{figure.caption.3}}
\newlabel{neuron}{{3.1}{7}{Schematic diagram of a McCulloch-Pitts neuron. The strength of the connection from neuron $ j $ to neuron $ i $ is denoted by $ w_{ij} $ \cite {mehlig}\relax }{figure.caption.3}{}}
\newlabel{local_field}{{3.6}{7}{McCulloch-Pitts neurons}{equation.3.1.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Activation functions}{7}{subsection.3.1.3}}
\citation{groman}
\citation{stanford-github}
\citation{stanford-github}
\citation{groman}
\@writefile{toc}{\contentsline {subsubsection}{Sigmoid}{8}{section*.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sigmoid function and its derivative. Notice that the derivative goes to zero very soon.\relax }}{8}{figure.caption.5}}
\citation{stanford-github}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Hyperbolic tangent}{9}{section*.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Hyperbolic tangent and its derivative.\relax }}{9}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsubsection}{Rectified linear unit (ReLu)}{9}{section*.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces ReLu and its derivative. ReLu does not saturate!\relax }}{9}{figure.caption.9}}
\citation{stanford-L4}
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLu}{10}{section*.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Leaky ReLu and its derivative.\relax }}{10}{figure.caption.11}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Multilayer perceptrons}{11}{subsection.3.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Perceptron with one hidden layer \cite  {mehlig}\relax }}{11}{figure.caption.12}}
\newlabel{perceptron}{{3.6}{11}{Perceptron with one hidden layer \cite {mehlig}\relax }{figure.caption.12}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Output classifier - softmax}{12}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Softmax classifier: the neurons in this layer are not independent \cite  {mehlig}\relax }}{12}{figure.caption.14}}
\newlabel{softmax}{{3.7}{12}{Softmax classifier: the neurons in this layer are not independent \cite {mehlig}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{Linear separability}{12}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Linearly separable (left) and not linearly separable problems (right). The decision boundary needs to be piece-wise linear for the not linearly separable problem \cite  {mehlig}\relax }}{12}{figure.caption.16}}
\newlabel{separability}{{3.8}{12}{Linearly separable (left) and not linearly separable problems (right). The decision boundary needs to be piece-wise linear for the not linearly separable problem \cite {mehlig}\relax }{figure.caption.16}{}}
\citation{groman}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Training of artificial neural networks}{14}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Loss function}{14}{subsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Mean Squared Error (MSE)}{14}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Negative Log Likelihood}{14}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{Cross Entropy Loss}{14}{section*.19}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Gradient optimization and backpropagation}{15}{subsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Backpropagation algorithm: the states of the neurons are updated forward (from left to right) while errors are updated backward (right to left) \cite  {mehlig}\relax }}{15}{figure.caption.20}}
\newlabel{backprop}{{3.9}{15}{Backpropagation algorithm: the states of the neurons are updated forward (from left to right) while errors are updated backward (right to left) \cite {mehlig}\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient descent}{15}{section*.21}}
\citation{coors}
\citation{coors}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Effect of the learning rate on optimization: the value must be chosen carefully for the algorithm to converge \cite  {coors}\relax }}{16}{figure.caption.22}}
\newlabel{learning_rate}{{3.10}{16}{Effect of the learning rate on optimization: the value must be chosen carefully for the algorithm to converge \cite {coors}\relax }{figure.caption.22}{}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{eniola}
\citation{mehlig}
\citation{stanford-L4}
\citation{mehlig}
\newlabel{update_rule}{{3.20}{17}{Gradient descent}{equation.3.2.20}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient descent\relax }}{17}{algorithm.1}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic gradient descent}{17}{section*.23}}
\@writefile{toc}{\contentsline {subsubsection}{Vanishing and exploding gradient problems}{17}{section*.24}}
\citation{stanford-L7}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-github}
\citation{stanford-L7}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Momentum}{18}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Momentum (left) and Nesterov's Momentum (right) \cite  {mehlig}\relax }}{18}{figure.caption.26}}
\newlabel{momentum}{{3.11}{18}{Momentum (left) and Nesterov's Momentum (right) \cite {mehlig}\relax }{figure.caption.26}{}}
\citation{stanford-L7}
\citation{stanford-L7}
\citation{groman}
\citation{bushaev}
\citation{groman}
\citation{groman}
\citation{groman}
\@writefile{toc}{\contentsline {subsubsection}{Other optimization algorithms}{19}{section*.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Comparison of different optimization algorithms \cite  {groman}\relax }}{19}{figure.caption.28}}
\newlabel{algorithms}{{3.12}{19}{Comparison of different optimization algorithms \cite {groman}\relax }{figure.caption.28}{}}
\citation{mehlig}
\citation{stanford-L6}
\citation{stanford-L6}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L6}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Improving training performance}{20}{subsection.3.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{Initialization of weights and thresholds}{20}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{Overfitting and regularization}{20}{section*.30}}
\citation{issue}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L7}
\citation{arvi}
\citation{arvi}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalisation}{21}{section*.31}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout}{21}{section*.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces ANN without (left) and with dropout (right) \cite  {arvi}\relax }}{21}{figure.caption.33}}
\newlabel{dropout}{{3.13}{21}{ANN without (left) and with dropout (right) \cite {arvi}\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data augmentation}{21}{section*.34}}
\@writefile{toc}{\contentsline {subsubsection}{Early stopping}{21}{section*.35}}
\citation{mehlig}
\citation{mehlig}
\citation{mehlig}
\citation{stanford-L7}
\citation{stanford-L7}
\citation{stanford-github}
\citation{mehlig}
\citation{mehlig}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Progress of training and validation losses. The plot is schematic, and the data is smoothed. The training is stopped when the validation energy begins to increase \cite  {mehlig}\relax }}{22}{figure.caption.36}}
\newlabel{dropout}{{3.14}{22}{Progress of training and validation losses. The plot is schematic, and the data is smoothed. The training is stopped when the validation energy begins to increase \cite {mehlig}\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning}{22}{section*.37}}
\@writefile{toc}{\contentsline {subsubsection}{Data pre-processing}{22}{section*.38}}
\citation{stanford-github}
\citation{stanford-github}
\citation{coors}
\citation{coors}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Convolutional neural networks}{23}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}CNN layer types}{23}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{Convolution layers}{23}{section*.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The full-depth convolution operation in a convolutional layer. The input size corresponds to a small RGB image. The result of the series of convolutions is a tensor of stacked activation maps for the filters used in the layer. \cite  {coors}\relax }}{23}{figure.caption.40}}
\newlabel{conv}{{3.15}{23}{The full-depth convolution operation in a convolutional layer. The input size corresponds to a small RGB image. The result of the series of convolutions is a tensor of stacked activation maps for the filters used in the layer. \cite {coors}\relax }{figure.caption.40}{}}
\citation{mehlig}
\citation{coors}
\citation{coors}
\citation{mehlig}
\citation{mehlig}
\@writefile{toc}{\contentsline {subsubsection}{Pooling layers}{24}{section*.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Max-pooling of size 2x2 and stride 2. \cite  {coors}\relax }}{24}{figure.caption.42}}
\newlabel{pool}{{3.16}{24}{Max-pooling of size 2x2 and stride 2. \cite {coors}\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fully connected layers (FCN)}{24}{section*.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Schematic of the standard CNN topology for image classification. \cite  {mehlig}\relax }}{24}{figure.caption.44}}
\newlabel{pool}{{3.17}{24}{Schematic of the standard CNN topology for image classification. \cite {mehlig}\relax }{figure.caption.44}{}}
\citation{stanford-github}
\citation{krizhevsky}
\citation{lecun}
\citation{stanford-github}
\citation{szegedy}
\citation{stanford-github}
\citation{vgg}
\citation{stanford-github}
\citation{resnet}
\citation{stanford-github}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Examples of CNN architectures}{25}{subsection.3.3.2}}
\citation{coufal}
\citation{segnet}
\citation{bayesian}
\citation{zeltner}
\citation{zeltner}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Semantic segmentation}{26}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Encoder-decoder architecture}{26}{subsection.3.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Example of encoder-decoder CNN architecture. \cite  {zeltner}\relax }}{26}{figure.caption.45}}
\newlabel{encoder}{{3.18}{26}{Example of encoder-decoder CNN architecture. \cite {zeltner}\relax }{figure.caption.45}{}}
\citation{segnet}
\citation{segnet}
\citation{segnet}
\citation{segnet_tut}
\citation{segnet_tut}
\@writefile{toc}{\contentsline {subsubsection}{Upsampling with transfer convolution and unpooling}{27}{section*.46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}SegNet}{27}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{SegNet - Encoder}{27}{section*.47}}
\@writefile{toc}{\contentsline {subsubsection}{SegNet - Decoder}{27}{section*.48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Bayesian SegNet}{28}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{Monte Carlo Dropout}{28}{section*.49}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluating Segmentation Performance}{28}{section*.50}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation and method}{29}{chapter.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}CPU vs. GPU for Training ANN}{29}{section.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{Tensor Cores}{29}{section*.51}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Libraries for ANN}{29}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Caffe}{30}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Setting up Environment for Caffe}{30}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Hardware Configuration}{30}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Software Configuration}{30}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Operating System}{30}{section*.52}}
\@writefile{toc}{\contentsline {subsubsection}{Enabling NVIDIA Driver}{31}{section*.53}}
\@writefile{toc}{\contentsline {subsubsection}{CUDA Installation}{31}{section*.54}}
\@writefile{toc}{\contentsline {subsubsection}{Installation of cuDNN}{32}{section*.55}}
\@writefile{toc}{\contentsline {subsubsection}{Setting up Python Editor}{32}{section*.56}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Building Caffe for SegNet}{33}{subsection.4.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Image Annotation}{34}{section.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{Labelbox}{34}{section*.57}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Setting up SegNet}{34}{section.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Solver Settings}{35}{subsection.4.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Training}{35}{subsection.4.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{Input Layer and Input pre-processing}{35}{section*.58}}
\@writefile{toc}{\contentsline {subsubsection}{Output Dimensions}{36}{section*.59}}
\@writefile{toc}{\contentsline {subsubsection}{Softmax Classifier}{37}{section*.60}}
\@writefile{toc}{\contentsline {subsubsection}{Training Initialization}{37}{section*.61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Inference}{39}{subsection.4.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{Calculating Statistics for Batch Normalisation}{39}{section*.62}}
\@writefile{toc}{\contentsline {subsubsection}{Running the Segmentation}{39}{section*.63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Testing}{40}{subsection.4.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Bayesian SegNet}{40}{subsection.4.5.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.6}SegNet Basic and Bayesian SegNet Basic}{42}{subsection.4.5.6}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Optimization of Hyperparameters}{42}{section.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{Cross-validation Strategy}{42}{section*.64}}
\@writefile{toc}{\contentsline {subsubsection}{Optimizer}{42}{section*.65}}
\@writefile{toc}{\contentsline {subsubsection}{Learning Rate}{42}{section*.66}}
\@writefile{toc}{\contentsline {subsubsection}{Regularisation}{43}{section*.67}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{44}{chapter.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Discussion and Future Work}{45}{chapter.6}}
\bibcite{mwiti}{1}
\bibcite{sergios}{2}
\bibcite{mehlig}{3}
\bibcite{santiago}{4}
\bibcite{goodfellow}{5}
\bibcite{groman}{6}
\bibcite{stanford-github}{7}
\bibcite{stanford-L4}{8}
\bibcite{stanford-L6}{9}
\bibcite{stanford-L7}{10}
\bibcite{coors}{11}
\bibcite{eniola}{12}
\bibcite{bushaev}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Bibliography}{46}{chapter.7}}
\bibcite{issue}{14}
\bibcite{arvi}{15}
\bibcite{krizhevsky}{16}
\bibcite{lecun}{17}
\bibcite{szegedy}{18}
\bibcite{vgg}{19}
\bibcite{resnet}{20}
\bibcite{coufal}{21}
\bibcite{bayesian}{22}
\bibcite{segnet}{23}
\bibcite{segnet_tut}{24}
\bibcite{zeltner}{25}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Seznam použitých zkratek a symbolů}{49}{chapter.8}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Seznam příloh}{50}{chapter.9}}
\gdef\pocetstran{50}
