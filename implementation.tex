\chapter{Implementation and method}

In this chapter, the original Caffe [] implementation of SegNet and Bayesian SegNet together with their simplified version SegNet Basic and Bayesian SegNet Basic will be tested on a custom dataset. Part of this will be evaluating the effect of various training hyperparameters, solvers, data augmentation techniques etc. This will also give the instructions on how to set up the software/hardware environment to run Caffe framework.

\section{CPU vs. GPU for Training ANN}

CPU is the main processing unit of a computer. Current CPU's usually have 4 to 8 separate cores, which allows them to run several tasks in parallel. Graphics processing unit (GPU) was originally designed for performing only rendering computer graphics. Table XY gives and idea of how these two computational units differ in terms of the kind of task they're designed for. Note that CPU has much lower number of cores, but these run at high frequency and are very capable in terms of the instructions they perform. Therefore, CPU's are great for sequential tasks. On the other hand, GPU comprises of a large number of 'simple' cores, which makes it better for computing parallel tasks. 

In terms of the available memory, CPU doesn't have its own resources (apart from the very small memory sections called caches) and has access to the system's RAM, whose size is very often between 8 and 32 GB for powerful PC's. GPU's, on the other hand, have their own block of RAM on the chip because the access top the main system's RAM has usually many bottlenecks. The size of the RAM for the top-end GPU's ranges from 8 to 12 GB.

The main part of the computations in Neural Networks in general is matrix multiplication. For this, GPU has the power of performing these operations by parts in parallel which speeds up the training significantly. 

There have been created abstraction frameworks such as CUDA and OpenCL, that allow programmers do write their code in an usual manner and run it directly on GPU. For the purposes of Neral Networks, NVIDIA has developed a library of the most commonly used CUDA primitives named cuDNN. 

\subsubsection{Tensor Cores}

Tensor Core is a special GPU feature offered by NVIDIA cards. It enables mixed-precision computing, dynamically adapting calculations to accelerate throughput while preserving accuracy. The latest generation expands these speedups to a full range of workloads. From 10x speedups in AI training with Tensor Float 32 data type, to 2.5x boosts for high-performance computing with floating point 64 (double precision). [nvidia site]

\section{ANN frameworks}

As the architecture and training of Neural Networks are getting more complicated, there is a room for programmers to make ANN frameworks such as Caffe, TensorFlow, and PyTorch as user friendly as possible. The idea of these software tools is to make a higher abstraction of the architecture of the network called computational graph. The user can therefore think of designing and training the network separately by applying an optimizer to the computational graph that represents the layers of the network. 

Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors. [berkeley caffe] The main difference from the other mentioned frameworks is that the user often doesn't need to write any code at all. The architecture of the network (the computational graph) is created in a .prototxt file, which is a standard text file in which one fills in the subsequent layers of the network in the desired order. Also, rather than having a optimizer object, one creates another .prototxt file that contains parameters such as the optimizer type (SGD, Adam, etc.), learning rate, momentum constant and others. After both of this files are created, the user runs Caffe computation from the command line. The core of the framework is written in C++. Pre-built binaries are called When the computation is started.

Caffe also has bindings for Python (CPW - Caffe Python Wrapper) and Matlab, which if very useful for evaluating the training statistics. 

\section{Setting up Environment for Caffe}

\subsection{Hardware configuration}

The GPU used for the computations has been picked according to the most up-to-date benchmarks and recommendations found online [source]. When choosing GPUs in general, one needs to decide between ATI/AMD and NVIDIA chips. For this case however, NVIDIA is the choice because it's way more 'ANN-friendly' as it's offering more features specifically designed for ANN computations. 

It is also advisable to use SSD in the PC configuration, because the data flow begins from reading the training data (images) from a storage, in this case from the computer's hard drive. Another way is moving the training data into RAM before the training is initiated [source, dalasi info]. 

Table XY shows the complete PC specifications used for training SegNet for the purposes of this thesis.  

\subsection{Software configuration} 

\subsubsection{Operating System} 

The standard platform for running Caffe is Ubuntu, which is a Linux distribution from Cannonical based on Debian. The environment used is Ubuntu 18.04 LTS 64 bit. Is is important to let the Ubuntu installer download the latest updates, or, after the installation, invoke the update command to ensure that the most up-to-date packages will be installed. For this, one can call

\begin{lstlisting}[language=bash]
$ sudo apt update
$ sudo apt upgrade
\end{lstlisting}

\subsubsection{Enabling NVIDIA driver}

Ubuntu 18.04 enables the default Nouveau graphics driver after installation. Before taking other steps, it is vital to disable the Nouveau driver and use NVIDIA instead in \textit{Application menu -> Software \& Updates -> Additional drivers
	-> Using NVIDIA driver metapackage from nvidia-driver-XYZ (proprietary, tested) -> Apply changes.} The driver version used is nvidia-driver-440.

[https://www.linuxbabe.com/ubuntu/install-nvidia-driver-ubuntu-18-04]

\subsubsection{CUDA installation}

CUDA version is determined by the version of cuDNN compatible with the used Caffe version, which is cuDNN 5.1 in this case. The corresponding CUDA version is CUDA 8.0. On Ubuntu 18.04, the procedure is the following:

\begin{itemize}
	\item \textbf{Download CUDA 8.0 runfile.} Go to \href{https://developer.nvidia.com/cuda-80-ga2-download-archive}{CUDA Legacy Releases} and look for 'CUDA Toolkit 8.0 GA2 (Feb 2017)'. The standard .deb installer support only Ubuntu 16.04 LTS and therefore the installation must be performed via the runfile method. Navigate to Linux -> x86\_64 -> Ubuntu -> 16.04 -> runfile (local) -> Base installer. Also, download the Patch file. 
	
	\item \textbf{Perform the runfile installation of CUDA.} Open the Ubuntu Terminal (Ctrl+Alt+T) and run
	
	\begin{lstlisting}[language=bash]
	$ cd /path/to/cuda_8.0.61_375.26_linux.run # Navigates to folder with CUDA
	$ sudo chmod a+x cuda*		# Makes the cuda*.run executable
	$ ./cuda*.run --tar mxvf 	# Unpacks the .runfile content
	$ sudo cp InstallUtils.pm /usr/lib/x86_64-linux-gnu/perl-base  # Copy one of the extracted files to perl-base
	$ sudo sh cuda_8.0.61_375.26_linux.run --override # Start the installation 
		# The licence agreement
		$ accept 
		# You are attempting to install on an unsupported configuration. Do you wish to continue?
		$ yes 
		# Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 375.26?
		$ no
		# Install the CUDA 8.0 Toolkit?
		$ yes 
		$ <press enter> (leave deafult location)
		# Do you want to install a symbolic link at /usr/local/cuda?
		$ yes
		# Install the CUDA 8.0 Samples?
		$ no
	\end{lstlisting}
	
	After the installation is done, ignore the '***WARNING: Incomplete installation!' statement, because the NVIDIA driver is already installed. 
	
	After the installation of the CUDA 8.0 Base package is done, run the Patch installation is a similar fashion:
	
	\begin{lstlisting}[language=bash]
	$ sudo sh cuda_8.0.61.2_linux.run
	\end{lstlisting}
	
	\item \textbf{Perform the post-installation actions.} The system needs to know the location of CUDA executables. The common way is to set these "PATH" variables in the current session of the Terminal. However, it's useful to add these permanently to '\textasciitilde{}/.bashrc' :
		
	\begin{lstlisting}[language=bash]
	$ sudo gedit ~/.bashrc # Opens the .bashrc file in text editor
	\end{lstlisting}
	
	In the text editor, append the following two statements to the end of the file:
	
	\begin{lstlisting}[language=bash]
	export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}
	export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\
			${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
	\end{lstlisting}	
	
	From this point, all newly opened Terminal sessions should have the paths set correctly. 
	
\end{itemize}

\subsubsection{cuDNN installation}

The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. It provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. [https://developer.nvidia.com/cudnn] 

\begin{itemize}
	\item \textbf{Download cuDNN 5.1 for CUDA 8.0.} To get the appropriate cuDNN version for Caffe and CUDA 8.0, go to \href{https://developer.nvidia.com/rdp/cudnn-archive}{cuDNN Archive} (requires login) and look for \textit{Download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0 -> cuDNN v5.1 Library for Linux}. Extract the archive, navigate to the extracted folder and copy the files to the CUDA 8.0 installation folder:
	
	\begin{lstlisting}[language=bash]
	$ tar -xf cudnn-8.0-linux-x64-v5.1.tgz 
	$ cd cuda
	$ sudo cp -a include/cudnn.h /usr/local/cuda/include/
	$ sudo cp -a lib64/libcudnn* /usr/local/cuda/lib64/
	\end{lstlisting}	
\end{itemize}

\subsubsection{Building Caffe for SegNet} 

The Caffe code is an open-source software. The authors of the SegNet created a slightly modified version of Caffe (caffe-segnet) that supports some special SegNet features, such as custom layer types. In addition, since the original caffe-segnet supports just cuDNN v2, which is not supported for new pascal based GPUs, there's another version of caffe-segnet from [TimmoSaemannGithub] that supports cuDNN 5.1 and decreases the inference time by 25 \% to 35 \%. This version has therefore been selected for running SegNet. From this point on, the term 'Caffe' will be equivalent to 'caffe-segnet' in the text.

\begin{itemize}
		
	\item \textbf{Install Caffe dependencies.} Caffe is available as a source code and therefore needs to be compiled on the target platform. For this, several steps need to be taken to ensure that all libraries are available during the build. 
	
	\begin{lstlisting}[language=bash]
	$ sudo apt install python3-opencv 			# OpenCV, version 3
	$ sudo apt-get install libatlas-base-dev 	# Atlas BLAS library
	$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler
	$ sudo apt-get install libboost-all-dev		# Boost
	$ sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev
	$ sudo apt-get install python3-pip
	$ sudo pip3 install protobuf
	$ sudo apt-get install the python3-dev
	\end{lstlisting}
	
	\item \textbf{Download Caffe (caffe-segnet-cudnn5) source code.} Go to \href{https://github.com/TimoSaemann/caffe-segnet-cudnn5}{Timmoe Saemann's Github repository} and clone/download it. 
	\item \textbf{Set the build configuration file.} The build is done via the 'make' command, which needs the 'Makefile.config' file to be present in the parent directory ('caffe-segnet-cudnn5-master'). This file contains the build options and needs to be configured properly. Fortunately, the correct form of 'Makefile.config' is part of this thesis and can be found in the Attachment XY. 
	
	\item \textbf{Install gcc/g++ compliers.} The CUDA/cuDNN libraries used during the build are compatible only with gcc/g++ compilers of version 5. To install these, run:
	
	\begin{lstlisting}[language=bash]
	$ sudo apt install gcc-5 g++-5
	# Create symbolic links so CUDA can see the proper compiler binaries
	$ sudo ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc
	$ sudo ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++
	\end{lstlisting}
	
	\item \textbf{Start the build.} Once the 'Makefile.config' file is located in the 'caffe-segnet-cudnn5-master', everything should be ready for the final step. Type these commands to initiate and test the Caffe build (don't forget to build pycaffe (Caffe Python Wrapper)):
	
	\begin{lstlisting}[language=bash]
	make all -j4	# start build
	make test -j4	# test build
	make runtest	# run Caffe and test it
	make pycaffe	# build pycaffe 
	\end{lstlisting} 	
\end{itemize}
[https://mc.ai/installing-caffe-on-ubuntu-18-04-with-cuda-and-cudnn/]

\subsubsection{Installing and Setting Python environment????}

Neni to uz moc podrobny? Zas na druhou stranu, jednoduchy to neni a je to potreba mit nastaveny spravne aby se v tom dobre delalo a vsechno fungovalo...

\section{Data Preparation}

In supervised learning, one needs to manually create the training data consisting of inputs and corresponding targets (in segmentation, ground truths). There's a variety of annotation tools available on the internet, both under commercial and free licenses. 

\subsubsection{Labelbox}

Labelbox in a paid online annotation tool capable of creating labels for semantic segmentation. The best feature of Labelbox is that it allows to share the datasets with other users which can speed up the labeling significantly. Labelbox offers free access to the full version to students. When the labeling is done, one needs to export the paths to the image/label pairs to s .JSON file. This file contains URLs to the images that are stored online and it is neccessary to download them separately (Labelbox is still in development, this is valid by the time of publishing the thesis). To automate this process, one can call the function 'download()' from the 'utils.py' file. 

The training images and their ground truths are stored as .jpg and .png files. The corresponding pairs are denoted as rows in the 'image\_paths.txt' file in the format 

$$
\text{'/path/to/image.jpg /path/to/label.png'}
$$

In 'utils.py', this corresponds to the function 'make\_ txt()'. The script will also make separate directories for training, testing and validation datasets by calling 'make\_dirs.py'.

\section{Setting up SegNet}

\subsubsection{Optimization file}

The file 'solver.prototxt' contains the optimization parameters. The detailed description of the parameters can be found on the original Caffe website. The basic description can be found in the snippet below. 

\begin{lstlisting}

// Training file
net: "/path/to/train.prototxt"	
// Caffe GPU version
solver_mode: GPU
// Solver type		
type: "AdaDelta"
// Initial learning rate, changes according to lr_policy		
base_lr: 0.061		
// Drops the learning rate as 'gamma*base_lr' every 'stepsize' iterations
lr_policy: "step"		 	
gamma: 1.0
stepsize: 10000000

// Show loss and accuracy every 'display' iterations
display: 130
// Max number of iteration. One iteration = a pass of one mini-batch			
max_iter: 3000	
// Regularization technique called Weight decay		
weight_decay: 0.00053	
// Saves the weights after 'snapshot' iterations
snapshot: 1000000		
snapshot_prefix: "/path/to/snap" 
// Optional, for accumulating gradients due to low GPU memory 
#iter_size: 4			

test_initialization: false	
test_iter: 1
test_interval: 100000000

\end{lstlisting}

\subsubsection{Training file}

SegNet implenetation comes as three files. 'segnet\_train.prototxt' is a file for the training phase. It begins with the data import layer. In this case, images and labels are loaded as .jpg and .png files directly from the hard drive. The path to the 'image\_paths.txt' file containg the image/label paths is entered as the 'source' parameter of the 'DenseImageData' layer in Caffe.

This layer also specifies the size of the mini-batch. This value is limited by the amount of memory the GPU offers. When a larger size of the mini-batch is needed, one solution Caffe offers is to specify the 'iter\_size' parameter in the Solver file. The total mini-batch size in Caffe is always a result of $iter\_size \cdot batch\_size$. By default, the value of 'iter\_size' is set to 1.

The 'shuffle' parameter in the Data layer determines whether the training dataset is shuffled after each epoch. This is usually desirable as it helps the optimization algorithm by bringing another stochasticity into the computation. 

The last important parameter is the 'mirror' parameter, which applies random mirrors to the input data and hence augments the dataset. If one needs to apply more complex data augmentation techniques, it's necessary to perform them separately and feed the Data layer with already processed images.

The encoder and decoder weights are all initialized using MSRA method.  

\begin{lstlisting}

// The first layer in the network

name: "bayesian_segnet_train"
layer {
	name: "data"
	type: "DenseImageData"
	top: "data"
	top: "label"
	dense_image_data_param {
		source: "/SegNet_navganti/data/custom/train_linux.txt"
		batch_size: 4   			    			
		shuffle: true
		mirror: true	
	}
	
\end{lstlisting}	 

In the original version, SegNet segments 11 classes. This corresponds to the pixel values in the PNG label files starting from zero, for instance, segmentation mask for class number 1 has a pixel value 0 in the label file, etc. However, the goal of this thesis is to set the network to segment only two classes - \textbf{PATH, BACKGROUND}. To change the size of the output classifier, it is necessary to change the output dimension of the last layer:

\begin{lstlisting}

// The last conv layer in the network

layer {
	bottom: "conv1_2_D"
	top: "conv1_1_D"
	name: "conv1_1_D"
	type: "Convolution"
	param {
		lr_mult: 1
		decay_mult: 1
	}
	param {
		lr_mult: 2
		decay_mult: 0
	}
	convolution_param {
		weight_filler {
			type: "msra"
		}
		bias_filler {
			type: "constant"
		}
		num_output: 2		// Set this to the number of classes
		pad: 1
		kernel_size: 3
	}
}

\end{lstlisting}

When there is large variation in the number of
pixels in each class in the training set (e.g road, sky and building
pixels dominate the dataset) then there is a need to weight
the loss differently based on the true class. This is called class
balancing. The authors of SegNet use median frequency balancing [odkaz] where the weight assigned to a class in the loss function is the ratio of the
median of class frequencies computed on the entire training set
divided by the class frequency. This implies that larger classes in
the training set have a weight smaller than 1 and the weights
of the smallest classes are the highest. When no re-weighting is applied, we talk about natural frequency balancing. [doslovna citace??? segnet paper]

SegNet uses the cross-entropy loss as the loss function for
training the network. In Caffe, median frequency balancing is available via the 'weight\_by\_label\_freqs' parameter in the Softmax layer. Since the dataset used in the thesis has only 2 classes whose occurence can be considered as balanced, this option is set to 'false'. 

\begin{lstlisting}

layer {
	name: "loss"
	type: "SoftmaxWithLoss"
	bottom: "conv1_1_D"
	bottom: "label"
	top: "loss"
	softmax_param {engine: CAFFE}
	loss_param: {
		weight_by_label_freqs: false	     
	}
}

\end{lstlisting}

To initiate the training from the command line, it is neccesary to first navigate to the folder with Caffe binaries, for example

\begin{lstlisting}[language=bash]
$ cd /SegNet/caffe-segnet/build/tools/
\end{lstlisting}

Then the training is initiated by executing

\begin{lstlisting}[language=bash]
$ ./caffe train -solver /SegNet/Models/segnet_solver.prototxt
\end{lstlisting}

Caffe allows the user to resume the training from a solver checkpoint (snapshot). To do that, one needs to call

\begin{lstlisting}[language=bash]
$ ./caffe train -solver /path/to/solver.prototxt -snapshot /path/to/snapshot_iter_XY.solverstate
\end{lstlisting}

Another scenario is when it's desired to use transfer learning. In this case, Caffe needs a path to the weight file of the pre-trained network. The corresponding command would be

\begin{lstlisting}[language=bash]
$ ./caffe train -solver /path/to/solver.prototxt -weights /path/to/pre_trained_weights.caffemodel
\end{lstlisting}


 [https://arxiv.org/pdf/1411.4734.pdf]

\subsubsection{Inference file}

Once the weights are obtained from the training phase, the Inference file comes into play. The structure of the network remains the same, apart from the input and output layers. The snippet below shows how the output changes: the loss function is no longer computed, the only output we care about are the Softmax probabilities. 

The Batch Normalisation layers [3] in SegNet shift the input feature maps according to their mean and variance statistics for each mini batch during training. At inference time we must use the statistics for the entire dataset. 
We first use the 'compute\_bn\_statistics.py' script that comes together with SegNet. These scripts are meant to be run from the command line and need to get command line parameters. 

The script computes the final model weights for the inference. The location of the 'weights.caffemodel' file is an input to the script 'segnet\_inference'. This script uses the CPW to read data from the layers of deployed Caffe SegNet. 

\subsubsection{Evaluating Segmentation Performance}

The performance of semantic segmentation is often described by so called IoU (intercestion over union) metrics. IoU is the area of overlap between the predicted segmentation and the ground truth divided by the area of union between the predicted segmentation and the ground truth, as shown in Figure XY. This metric ranges from 0–1 (0–100\%) with 0 signifying no overlap and 1 signifying perfectly overlapping segmentation.

[https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2]

[https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/]




http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html

\subsubsection{Setting up Virtualenv in Pycharm}



\begin{lstlisting}

layer {
	bottom: "conv1_2_D"
	top: "conv1_1_D"
	name: "conv1_1_D"
	type: "Convolution"
	param {
		lr_mult: 1
		decay_mult: 1
	}
	param {
		lr_mult: 2
		decay_mult: 0
	}
	convolution_param {
		weight_filler {
			type: "msra"
		}
		bias_filler {
			type: "constant"
			value: 0
		}
		num_output: 2
		pad: 1
		kernel_size: 3
	}
}
layer {
	name: "prob"
	type: "Softmax"
	bottom: "conv1_1_D"
	top: "prob"
	softmax_param {engine: CAFFE}
}

\end{lstlisting}

\chapter{Results}

TRANSFER LEARNING + BATCH NORMALIZATION


\newpage
--------------TRAINING CAFFE NOTES---------------

We perform local contrast normalization [54] to the RGB input. 
We train the model with dropout and sample the posterior distribution over the weights at test time using dropout
to obtain the posterior distribution of softmax class probabilities. We take the mean of these samples for our segmentation prediction and use the variance to output model
uncertainty for each class. We take the mean of the per class
variance measurements as an overall measure of model uncertainty. We also explored using the variation ratio as
a measure of uncertainty (i.e. the percentage of samples
which agree with the class prediction) however we found
this to qualitatively produce a more binary measure of
model uncertainty. Fig. 2 shows a schematic of the segmentation prediction and model uncertainty estimate process.