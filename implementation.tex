\chapter{Implementation and method}

In this chapter, the original Caffe implementation of SegNet and Bayesian SegNet with their simplified versions SegNet Basic and Bayesian SegNet Basic will be tested on a custom dataset. Part of this will be evaluating the effect of various hyperparameters on training. It will also give the instructions on how to set up the software and hardware environments to run Caffe library for ANN.

The entire network architecture and other code used in this section are available at \cite{filip_github}.

\section{CPU vs. GPU for training ANN}

Central Processing Unit (CPU) is the main computational unit of a computer and is designed to perform a wide variety of complex instructions. Current CPUs usually have 4 to 8 separate cores, which allow them to run several tasks in parallel. Graphics processing unit (GPU), on the other hand, was originally designed only for rendering computer graphics. Table XY gives an idea of how these two computational units differ in terms of the tasks they are designed for. Note that CPU has much lower number of cores, but these run at high frequency and are very capable in terms of the instructions they perform. Therefore, CPU's are great for sequential tasks. GPU comprises of a large number of 'simple' cores, which makes it more suitable for computing parallel tasks. 

The main part of the computations in ANN in general is matrix multiplication where GPU has the power of performing these operations by parts in parallel and speeds up the training significantly.

There have been created libraries such as CUDA and OpenCL that allow programmers to write their code in an usual manner and run it directly on a GPU. For the purposes of ANN, NVIDIA has also developed a library of the most commonly used CUDA primitives named cuDNN. 
 
CPU does not have its own memory resources (apart from very small memory sections called caches) and only has access to the system's RAM. External GPUs always come with their own block of RAM on the chip. The size of the RAM for the top-end GPUs ranges from 8 to 12 GB. When using GPUs to train ANN, the size of the RAM is crucial because the model with all its parameters resides in this memory. 

\subsubsection{Tensor cores}

Tensor Core is a special GPU feature offered by NVIDIA cards. It enables mixed-precision computing, dynamically adapting calculations to accelerate throughput while preserving accuracy. The latest generation expands these speedups to a full range of workloads. From 10x speedups in AI training with Tensor Float 32 data type, to 2.5x boosts for high-performance computing with floating point 64 (double precision). \cite{nvidia}

\newpage
\section{Libraries for ANN}

As the architecture and training of ANN are getting more complex, it is very helpful to use programming tools with higher abstraction when designing them. For this there are libraries such as Caffe, TensorFlow, and PyTorch. The common idea of these libraries is to make an abstraction of the network's architecture called computational graph. Therefore, the user can think of designing and training the network separately by applying an optimizer to the computational graph that represents the network's layers. 

\subsection{Caffe}

Caffe is a deep learning library made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors. \cite{caffe} The main difference from other libraries is that the user often doesn't need to write any code at all. The architecture of the network (the computational graph) is described in a \textit{.prototxt} file where one creates the layers of the network in the desired order. Also, rather than having an optimizer object (in Tensorflow for example), one creates another \textit{.prototxt} file that contains parameters such as the optimizer type (SGD, Adam, etc.), learning rate, momentum constant and others. After both of these files are created, the user runs Caffe computation from the command line. The library is written in C++ and the pre-built binaries are called when the computation is executed.

Caffe comes with bindings for Python (CPW - Caffe Python Wrapper, or pycaffe) and Matlab, which if very useful for the inference phase. The biggest downside of Caffe and CPW is that they are very poorly documented.

\section{Setting up environment for Caffe}

\subsection{Hardware configuration}

The GPU used for the computations has been selected according to the most up-to-date benchmarks and recommendations found online. When choosing a GPU in general, one needs to decide between AMD and NVIDIA chips. For ANN however, NVIDIA is the default choice because it's way more 'ANN-friendly' as it offers more features specifically designed for ANN computations. 

It's advisable to use a SSD in the training PC, because the data flow begins with reading the training data (images) from a storage, in this case from the computer's hard drive. Another possibility some libraries offer is moving the training data into RAM before the training is initiated. 

Table XY shows the complete PC specifications used for training SegNet. 

\newpage
\subsection{Software configuration} 

\subsubsection{Operating system} 

The standard platform for running Caffe is Ubuntu, which is a Linux distribution from Cannonical based on Debian. The environment used was Ubuntu 18.04 LTS 64 bit. It is important to let the Ubuntu installer download the latest updates, or, after the installation, invoke the update command to ensure that the most up-to-date packages will be installed. For this, one can call:

\begin{lstlisting}[language=bash]
$ sudo apt update
$ sudo apt upgrade
\end{lstlisting}

\subsubsection{Enabling NVIDIA driver}

Ubuntu 18.04 enables the default Nouveau graphics driver after the installation. Before taking other steps, it is \textbf{vital} to disable the Nouveau driver and use the NVIDIA instead. This is done by navigating to \textit{Application menu -> Software \& Updates -> Additional drivers
	-> Using NVIDIA driver metapackage from nvidia-driver-XYZ (proprietary, tested) -> Apply changes.} The driver version used was \textbf{nvidia-driver-440}.

%[https://www.linuxbabe.com/ubuntu/install-nvidia-driver-ubuntu-18-04]

\subsubsection{CUDA installation}

CUDA version is determined by the version of cuDNN compatible with the used Caffe version, which is cuDNN 5.1 in our case. The corresponding CUDA version is CUDA 8.0. On Ubuntu 18.04, the procedure is the following:

\begin{itemize}
	\item \textbf{Download CUDA 8.0 runfile.} Go to \href{https://developer.nvidia.com/cuda-80-ga2-download-archive}{CUDA Legacy Releases} and look for \textit{CUDA Toolkit 8.0 GA2 (Feb 2017}). The standard \textit{.deb} installer support only Ubuntu 16.04 LTS. Therefore, the installation must be performed via the runfile method. Navigate to \textit{Linux -> x86\_64 -> Ubuntu -> 16.04 -> runfile (local) -> Base installer}. Also, download the Patch file. 
	
	\item \textbf{Perform the runfile installation of CUDA.} Open the Ubuntu Terminal and run: \cite{nvidia_dev}
	
	\begin{lstlisting}[language=bash]
	$ cd /path/to/cuda_8.0.61_375.26_linux.run # Navigates to folder with CUDA
	$ sudo chmod a+x cuda*		# Makes the cuda*.run executable
	$ ./cuda*.run --tar mxvf 	# Unpacks the .runfile content
	$ sudo cp InstallUtils.pm /usr/lib/x86_64-linux-gnu/perl-base  # Copy one of the extracted files to perl-base
	$ sudo sh cuda_8.0.61_375.26_linux.run --override # Start the installation 
		# The licence agreement
		$ accept 
		# You are attempting to install on an unsupported configuration. Do you wish to continue?
		$ yes 
		# Install NVIDIA Accelerated Graphics Driver for Linux-x86_64 375.26?
		$ no
		# Install the CUDA 8.0 Toolkit?
		$ yes 
		$ <press enter> (leave deafult location)
		# Do you want to install a symbolic link at /usr/local/cuda?
		$ yes
		# Install the CUDA 8.0 Samples?
		$ no
	\end{lstlisting}
	
	After the installation is done, ignore the \textit{***WARNING: Incomplete installation!} statement, because the NVIDIA driver is already installed. 
	
	Now run the CUDA 8.0 Patch 2 installation in a similar fashion:
	
	\begin{lstlisting}[language=bash]
	$ sudo sh cuda_8.0.61.2_linux.run
	\end{lstlisting}
	
	\item \textbf{Perform the post-installation actions.} The system needs to know the location of CUDA executables. The common way is to set these "PATH" variables in the current session of the Ubuntu Terminal. However, it's useful to add these permanently to system's \textit{\textasciitilde{}/.bashrc} file:
		
	\begin{lstlisting}[language=bash]
	$ sudo gedit ~/.bashrc # Opens the .bashrc file in text editor
	\end{lstlisting}
	
	In the text editor, append the following two statements to the end of the file:
	
	\begin{lstlisting}[language=bash]
	export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}
	export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64\
			${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
	\end{lstlisting}	
	
	From this point, all newly opened Terminal sessions should have the paths set correctly.
	
\end{itemize}

\subsubsection{Installation of cuDNN}

The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. It provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. \cite{nvidia_dev} 

\begin{itemize}
	\item \textbf{Download cuDNN 5.1 for CUDA 8.0.} To get the corresponding cuDNN version for Caffe and CUDA 8.0, go to \href{https://developer.nvidia.com/rdp/cudnn-archive}{cuDNN Archive} (requires login) and look for \textit{Download cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0 -> cuDNN v5.1 Library for Linux}. Extract the archive, navigate to the extracted folder and copy the files to the CUDA 8.0 installation folder: \cite{nvidia_dev} 
	
	\newpage
	\begin{lstlisting}[language=bash]
	$ tar -xf cudnn-8.0-linux-x64-v5.1.tgz 
	$ cd cuda
	$ sudo cp -a include/cudnn.h /usr/local/cuda/include/
	$ sudo cp -a lib64/libcudnn* /usr/local/cuda/lib64/
	\end{lstlisting}	
\end{itemize}

\subsubsection{Setting up Python editor}

The scripts for evaluating SegNet performace are written in Python. It is advisable to use Pycharm Community Edition for an editor, because it offers a very convenient combination of GUI and the standard command line environment.

A good practice is using Python Virtual Environment to easily maintain the required packages and to make the project transferable to another Linux PC. In Pycharm, one can do this in an active Pycharm project by navigating to \textit{File -> Settings -> Project -> Project Interpreter -> <wheel icon on the right> -> Add}. The standard choice is the \textit{Virtualenv Environment}. The Base interpreter location on a fresh Ubuntu installation is \textit{/usr/bin/python3.6}. When we click OK, Pycharm creates a \textit{venv} folder at the specified location that includes all package files we install.

When the \textit{virtualenv} is configured properly, it will automatically be activated when we enter the Ubuntu Terminal session by clicking on the \textit{Terminal} button located at the bottom bar of Pycharm window. From this Terminal, we will be launching all SegNet scripts and use it to install the required packages by calling:

\begin{lstlisting}[language=bash]
(venv) user@user:/current/path$ pip3 install <package-name>
\end{lstlisting}

\subsection{Building Caffe for SegNet} 

Caffe is an open-source library. The authors of the SegNet created a slightly modified version of Caffe called \textit{caffe-segnet} that supports special SegNet layer types (upsample, bn, dense\_image\_data and softmax\_with\_loss (with class weighting)).

In addition, since the original \textit{caffe-segnet} supports just cuDNN v2, which is not supported by newer GPUs, there's another version of \textit{caffe-segnet} from [TimmoSaemannGithub] that supports cuDNN 5.1. The author claims that it decreases the inference time by 25 \% to 35 \%. Therefore, this version was selected for running SegNet. From this point on, the term 'Caffe' will be equivalent to '\textit{caffe-segnet}' in the text.

\begin{itemize}
		
	\item \textbf{Install Caffe dependencies.} Caffe is available as a source code and needs to be compiled on the target platform. For this, several steps need to be taken to ensure that all libraries are available during the build: \cite{caffe}
	
	\begin{lstlisting}[language=bash]
	$ sudo apt install python3-opencv 			# OpenCV, version 3
	$ sudo apt-get install libatlas-base-dev 	# Atlas BLAS library
	$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler
	$ sudo apt-get install libboost-all-dev		# Boost
	$ sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev
	$ sudo apt-get install python3-pip
	$ sudo pip3 install protobuf
	$ sudo apt-get install the python3-dev
	\end{lstlisting}
	
	\item \textbf{Download Caffe (caffe-segnet-cudnn5) source code.} Go to \cite{filip_github_caffe} and clone/download it. 
	\item \textbf{Set the build configuration file.} The build is done via the \textit{make} command, which needs the \textit{Makefile.config} file to be present in the parent directory (\textit{caffe-segnet-cudnn5-master}). This file contains the build options and needs to be configured properly. Fortunately, the correct form of \textit{Makefile.config} is part of this thesis and can be found in Attachment XY. 
	
	\item \textbf{Install gcc/g++ compliers.} The CUDA/cuDNN libraries used during the build are compatible only with gcc/g++ compilers of version 5. To install these, run:
	
	\begin{lstlisting}[language=bash]
	$ sudo apt install gcc-5 g++-5
	# Create symbolic links so CUDA can see the proper compiler binaries
	$ sudo ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc
	$ sudo ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++
	\end{lstlisting}
	
	\item \textbf{Initiate the build.} Once the \textit{Makefile.config} is located in the \textit{caffe-segnet-cudnn5-master} directory, everything should be ready for the final step. Execute these commands to initiate and test the Caffe build (don't forget to build pycaffe (Caffe Python Wrapper)):
	
	\begin{lstlisting}[language=bash]
	make all -j4	# start build
	make test -j4	# test build
	make runtest	# run Caffe and test it
	make pycaffe	# build pycaffe 
	\end{lstlisting} 	
\end{itemize}





