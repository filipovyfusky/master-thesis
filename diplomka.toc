\babel@toc {english}{}
\contentsline {chapter}{\numberline {1}Introduction}{2}{chapter.1}
\contentsline {chapter}{\numberline {2}Problem statements}{3}{chapter.2}
\contentsline {chapter}{\numberline {3}Research and theory}{4}{chapter.3}
\contentsline {section}{\numberline {3.1}Architecture of artificial neural networks}{4}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Feed-forward networks}{4}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}McCulloch-Pitts neurons}{5}{subsection.3.1.2}
\contentsline {subsection}{\numberline {3.1.3}Activation functions}{6}{subsection.3.1.3}
\contentsline {subsection}{\numberline {3.1.4}Multilayer perceptrons}{8}{subsection.3.1.4}
\contentsline {section}{\numberline {3.2}Training of artificial neural networks}{9}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Loss function}{9}{subsection.3.2.1}
\contentsline {subsection}{\numberline {3.2.2}Gradient descent and backpropagation}{10}{subsection.3.2.2}
\contentsline {subsubsection}{Gradient descent}{10}{section*.3}
\contentsline {subsubsection}{Vanishing gradient problem}{11}{section*.4}
\contentsline {subsubsection}{Stochastic gradient descent}{12}{section*.5}
\contentsline {subsection}{\numberline {3.2.3}Improving performance of optimization}{12}{subsection.3.2.3}
\contentsline {subsection}{\numberline {3.2.4}Other optimization algorithms}{12}{subsection.3.2.4}
\contentsline {subsubsection}{AdaDelta}{12}{section*.6}
\contentsline {subsubsection}{AdaGrad}{12}{section*.7}
\contentsline {subsubsection}{RMSprop}{12}{section*.8}
\contentsline {subsubsection}{Adam}{12}{section*.9}
\contentsline {chapter}{\numberline {4}Bibliography}{13}{chapter.4}
\contentsline {chapter}{\numberline {5}Seznam použitých zkratek a symbolů}{14}{chapter.5}
\contentsline {chapter}{\numberline {6}Seznam příloh}{15}{chapter.6}
