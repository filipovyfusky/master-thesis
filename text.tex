\chapter{Research and theory}
\label{research}
First part of this section gives an introduction to neural networks (NN). It begins by definition of fundamental terms needed to understand the core principles of NNs. Due to the fact that the research in this area is still heavily ongoing, the more advanced techniques described here may soon be out of date or replaced by better-performing ones and therefore the theoretical background is limited only to the extent relevant for the particular chosen network architecture. 

Second part presents some of the main approaches based on machine learning researches have recently used to tackle the semantic segmentation problem. However, not all of them use CNNs as the core algorithm. This part summarizes the main key points from the corresponding papers that contributed to this topic by presenting novel architectures and principles. It finishes by more detailed description of a method that is eventually found the most promising and thus selected for the final implementation.

\section{Architecture of artificial neural networks}
Artificial neural network algorithms are inspired by the architecture and the dynamics
of networks of neurons in human brain. They can learn to recognize structures in a given set of training data and generalize what they have learnt to other data sets (supervised learning). In supervised learning one uses a training data set of correct input/output pairs. One feeds an input from the training data into the input terminals of the network and compares the states of the output neurons to the target values. The network trainable parameters are changed as the training continues to minimize the differences between network outputs and targets for all input patterns in the training set. In this way the network learns to associate input patterns in the training set with the correct target values. A crucial question is whether the trained network can generalize: does it find the correct targets for input patterns that were not in the training set? 

\subsection{Feed-forward networks}

The goal of a feed-forward neural network is to find a non-linear, generally n-dimensional function that maps the space of the inputs x to the space of the outputs y. In other words, to learn the function [zdroj SANTIAGO]

$$ f^*: \mathbb{R}^m \rightarrow \mathbb{R}^n, f^*(x;\phi) $$

where $ \phi $ are trainable parameters of the network. The goal is to learn the value of the parameteres that result in the best function approximation, by solving the equation

$$ \phi \leftarrow \, arg \, min \, L(y, f^*(x;\phi)) $$

where $ L $ is a loss function chosen for the particular task. One can understand the term 'loss function' simply as a metric of 'how happy we are about the output that the network gives us for a given input' and therefore $f^*(x;\phi)$ is driven to match the ideal function $f(x;\phi)$ during network training. 

The structure of a feed-forward network is usually composed of many nested functions. For instance, there might be three functions $f^{(1)}$, $f^{(2)}$ and $f^{(3)}$ connected in a chain to the form

$$ f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x))) $$

These models are reffered to as feed-forward because information flows from the deepest nested function $f^{(1)}$ taking $ x $ as its direct input to other functions in the chain and finally to the output $ y $. One can name the functions starting by $f^{(1)}$ as the first layer (input layer) of the network, $f^{(2)}$ is called the second layer, and so on. The final layer of the network is called output layer. 

Recall that in supervised learning one needs a set of training data, in this case a set of matching x, y (footnote: y are often called LABELS) pairs. The training examples specify directly what the output layer must do at each point x; it must produce a value that is as close as possible to y. The behaviour of the other layers is not specified by the training data which is why we call these layers 'hidden layers'. In Figure XY , an image of a four-layer feed-forward neural network with two hidden layers can be seen.

It is now very important to highlight that the neural network must be seen as something that is capable of modeling practically any function we can think of [general approximation theorem]. The power of this brings us to the definition of a classification task. In this task, the function the network approximates has discrete states, true/false. To give the most typical example, let's consider we have a large set of black and white images containing hand-written digits from 0 to 9. Now we want the network to basically associate the information encoded in the pixel values of the input image with the information of what digit that particular image shows. In this case, the vector space of inputs (flattened image array in the simplest case of size m = width*height) is projected to the space of size n = 10 categories (digits 0 to 9). 

\subsection{McCulloch-Pitts neurons}

Layers in Figure XY can be further divided into distinct computational units (again, just another nested functions) called neurons. This is where the resemblance with/to biological neurons comes into play. The neurons are mathematically modelled as linear threshold units (McCulloch-Pitts neurons), they process all input signals coming to them from other neurons and compute an output. In its simplest form, the model for the artificial neuron has only two states, active or inactive. Let's stick with this simple model for a while. If the output exceeds a given threshold then the state of the neuron is said to be active, otherwise inactive. The model is illustrated in Figure 1.4. Neurons usually perform repeated computations, and one divides up time into discrete time steps $ t = 0,1,2,3,.... $ The state of neuron number $ j $ at time step $ t $ is denoted by

$$ 
n_j(t) = 
	\begin{cases}	
		0 & inactive,\\
		1 & active.
	\end{cases} 
$$

Given the signals $ n_j(t+1) $, neuron number i computes

$$  
n_j(t+1)=\theta_H \left(\sum\limits_{j}w_{ij}n_j(t) - \mu_i \right)
$$

As written, this computation is performed for all $ i $ neurons in parallel, and the outputs $ n_i $ are the inputs to all neurons at the next time step, therefore the outputs have the time argument $ t+1 $.

The weights wi j are called synaptic weights. Here the first index, i,
refers to the neuron that does the computation, and j labels all neurons that connect
to neuron i. The connection strengths between different pairs of neurons are in
general different, reflecting different strengths of the synaptic couplings.

The argument of $ \theta_H $ is often referred to as the local field

$$ b_i = \sum\limits_{j}w_{ij}n_j(t) - \mu_i $$

Equation (1.2) basically shows that the neuron performs a weighted linear average of the inputs nj and applies an offset (threshold) which is denoted by Âµi. Finally, the function $ \theta_H $ is reffered to as the activation function.

\subsection{Activation functions}

The general motivation for using activation functions is to bring non-linearity to the model. In the simplest case that has been discussed so far, the neurons can only have the states 0/1, which in terms of the activation function corresponds to the Heaviside function

$$ 
\theta_H(b) = 
\begin{cases}	
1 & \text{for $b > 0$,}\\
0 & \text{for $b < 0$.}
\end{cases} 
$$

In practice however, the simplest model must be generalized by allowing the neuron to respond continuously to its inputs, which is necessary for the optimization algorithms used in the training phase to perform well. To this end one replaces Eq. (1.2) by a general continuous activation function $ g(b) $. An example is shown in Figure 1.6. This dictates that the states assume continuous values too, not just the discrete values 0 and 1 as given in Equation (1.1).

There are several choices for the activation function which all come with their 'pros and cons' for a particular application the network is used for. However, there's a few requirements all of these functions should meet:

\begin{itemize}
	
\item \textbf{Nonlinearity}. As discussed above, the non-linearity is a general ability of a neural network allowing it to model very complex functions
\item \textbf{Monotocity and nondecreasibility}. This allows certain optimization algorithms to perform more stable as we'll see further.
\item \textbf{Differentiability (or at least piecewise differentiability)}. This is useful not only in terms of stability of the optimization algorithms, but also for the analytical derivation of the update rule for the network parameters during optimization. 
		
\end{itemize}

One also needs to distinguish between activation functions used in neurons in the input/hidden layers and neurons in the output layer. The reason for that comes from the definition of a classification task, where we would like to interpret the outputs of the network as 'probabilities' of the input belonging to a certain class. For this one can use commonly used Softmax classifier (function). In the example with hand-written digits, when we feed the network with an image showing digit 7, we want the network to spit out the 100 percent probability for the image belonging to the class 'digit 7' and 0 percent probability of it belonging to the classes 'digit 0', 'digit 1'... In such case, the output of the last layer of the network not only needs to be within 0 and 1, but in case of using Softmax the sum of all outputs must give unity as they are intepreted as (relative!) probabilities. We say 'relative' because the network's decision is only based on the features of this particular image differing in comparison with other data we fed in during training and does not reflect 'outer' probability at all. 

Another possibility for the output activation function is the sigmoid function. 

\begin{itemize}

\item \textbf{Sigmoid}

$$  
g(x)=\frac{1}{1 + e^{-x}}
$$

This function used to be broadly used mainly thank to the clear interpretation of the state of the neuron - active/inactive is represented by values 0/1. Sigmoid is currently obsolete for large networks. In short, it does not have optimal properties for the learning algorithm called 'backpropagation', which is to be discussed in next chapters. Also, the fact that its mean value in non-zero doesn't have a positive impact on the learning process either. [Groman]

%\item \textbf{Hyperbolic tangent }

%$$  
%g(x)=\tanh (x)
%$$

%Unlike Sigmoid, the range of its output is in the interval <-1,1>. However, it still has the same limitations as the previous function and therefore is not a suitable candidate for learning via backpropagation. 

\item \textbf{Rectified Linear unit (ReLu)}

$$
g(x)=\max (0,x)
$$

The authors of this concept found the inspiration in real biological neurons. The idea comes from a model for the relation between the electrical current through the cell membrane into the neuron cell, and the membrane potential. The main message is that there is a threshold below which the response of the neuron is strictly zero, as shown in Figure XY. The derivative of the ReLU function is discontinuous at $ x=0 $. A common convention is to set the derivative to zero at $ x=0 $.  

\item \textbf{Parametric (leaky) ReLu}

$$
g(x)=\max(x,\alpha x)
$$

By modifying the previously introduced function one gets a version of ReLu intended to address the biggest drawback of ReLu, which is the fact that some neurons may become dead (their output will be always zero) and thus not contribute to the network's output. Unfortunately there's generally no guarantee that using Leaky ReLu instead of ReLu will always mean better results. 

\item \textbf{Output activation function - Softmax}

The Softmax function is only used in output layer. This classifier differs from other activation functions by its dependency on other neurons in the layer

$$  
O_{i} = \frac{e^{\alpha b_i^{(L)}}}{\sum_{k=1}^{M} e^{\alpha b_k^{(L)}}}
$$

Here $ b_{i}^{(L)} = \sum_{j}w_{ij}^{(L)} V_{j}^{(L-1)} - \theta _{j}^{(L)} $  are the local fields of the neurons in the output layer. The constant $\alpha$ is usually taken to be unity. Let's repeat three important properties of softmax outputs: first that 
$ 0 \geq O_i \geq 1 $. Second, the values of the outputs sum to one $ \sum_{i=1} O_i = 1 $. This means that the outputs of softmax units can be interpreted as probabilities. Third, the outputs are monotonous: when $ b_i^{(L)} $ increases then $ O_i $ increases but the values $ O_k $ of the other output neurons $ k \neq i $ decrease.

\end{itemize}

\subsection{Multilayer perceptrons}

A perceptron is a layered feed-forward network. The left-most is the input layer. To the right follows a number of layers consisting of McCulloch-Pitts neurons. The right-most layer of neurons is the output layer where the output of the network is read out, usually as softmax probabilities. The other neuron layers are called hidden layers, their states are not read out directly. 

In perceptrons, all connections (weights) $ w_{ij} $ are one-way; every neuron (or input terminal) feeds only to neurons in the layer immediately to the right. There are no connections within layers, or back connections, or connections that jump over a layer. There are $ N $ input terminals. We denote the inputs coming to the input layer by

$$ x(\mu)= 
\begin{bmatrix}
x_{1}^{(\mu)} \\
x_{2}^{(\mu)} \\
\vdots \\
x_{N}^{(\mu)} 
\end{bmatrix}
$$

The index $ \mu $ labels different input patterns in the training set. The network shown in Figure XY would perform these computations

%$$
\begin{gather}
V_j^{(\mu)} = g(b_j^{(\mu)}) \quad \text{where} \quad b_j^{(\mu)} = \sum_{k} w_{jk} x_{k}^{(\mu)} - \theta_{j} \\
O_i^{(\mu)} = g(B_i^{(\mu)}) \quad \text{where} \quad B_i^{(\mu)} = \sum_{j} W_{ij} V_{j}^{(\mu)} - \Theta_{i} 
\end{gather}
%$$

Multilayer perceptron has generally N hidden layers. If it has more than two hidden layers, we usually start to talk about 'deep network'. 

The reason we use hidden layers is to tackle not linearly separable classification problems. Linear separability is shown in figure XY, where the input to the network is two-dimensional and we classify the input data into two classes (marked as black and white points in the graph). Classification problem is linearly separable if one is able to draw a single line (single plane in case of three inputs, etc.) to divide the input space into two distinct areas and hence solve the classification task. In general, the curve that separates the space into sub areas each representing a class is called the decision boundary. In the case shown in Figure XY, the line dividing the 2D space of inputs corresponds to the simplest possible case: a single neuron in the network.

An example of a not linearly separable task is shown in Figure XY. We need to divide the input space to more than two regions to solve the classification. The network corresponding to the case in Figure XY has one hidden layer with three neurons. By doing this we map the input space of size n = 2 to the hidden space of size m = 3 and use it as an input to other layers.

One can ask how many hidden layers and number of neurons should we use for a particular task? In short, the answer depends on how complicated the distribution of input patterns is.

The position of the decision boundary is determined by the values of weights and thresholds in the neurons (W and THR). These parameters are found by training the network. 

\section{Training of artificial neural networks}

Artificial neural networks are trained using iterative optimization algorithms. During training, one needs to choose the right loss function whose value goes to zero when the network produces the expected output. In each step of optimization, the trainable parameters are changed in order to achieve this. The effect each parameter has on the value of the loss function is determined by calculating the gradient of the loss function with respect to the particular parameter in the network. The way this information is used is then subject to the chosen algorithm. 

The principle explained above is in the neural networks theory known as backpropagation.

\subsection{Loss function}

Loss function is a metric of our happiness with the network's output. The choice depends on the nature of the task the network is used for and on the activation function used in the output layer. Here are the most commonly used ones:

\begin{itemize}
\item \textbf{Mean Squared Error}

$$ 	
L =  \frac{1}{2} \sum\limits_{\mu i} \left( t_{i}^{(\mu)} - O_{i}^{(\mu)} \right)^2
$$

MSE is used when the output layer of a network uses Sigmoid functions.

\item \textbf{Negative Log Likelihood} 

$$ 	
L = - \sum\limits_{\mu i}  t_{i}^{(\mu)} \ln (O_{i}^{(\mu)})
$$

Extensively used for classification tasks in the combination with Softmax classifier. 
\end{itemize}   

During training, the loss function is the one whose value is being optimized.

\subsection{Gradient descent and backpropagation}

Backpropagation is a way in which information about the correctness of the output flows through the network in order for the parameters to be adjusted. The scheme is shown in the network in Figure XY, where backpropagation is applied to a multilayer perceptron. 

The goal is to give the optimization algorithm values of gradients for all network parameters in each iteration. One needs to find partial derivatives of the loss funtion with respect to these parameters. In deep networks, one achieves this by applying the chain rule to the formula for calculating the loss function.

\subsubsection{Gradient descent}

The general formula for the gradient descent algorithm goes as follows:

$$ 
\delta \phi = - \eta \frac{\partial H}{\partial \phi}
$$

where $ \phi $ is the parameter we care about (weights, thresholds). In each iteration, we compute the derivative of the loss function with respect to all network parameters and thus get the increments $ \delta \phi $. Parameter $ \eta $ is called the learning rate and is always a small number greater than zero. This parameter determines the size of the step we take in the way of the steepest descent in the landscape (in case of two parameters) of the loss function. This is shown in Figure XY. We see that the behaviour and convergence of the algorithm is strongly dependent on choosing the learning rate value: if the steps are too small, the learning will be slow and we are more likely to end up in a local minimum. On the other hand, if the value of it is too big, the algorithm may even start to 'climb up the hill' and cause the loss function to grow. 

Given a multilayer perceptron with hidden layers and the Negative log likelihood loss function, the gradient descent (GD) algorithm for weight and threshold updates is defined as follows:

\begin{gather}
	\delta w_{mn} = - \eta \frac{\partial L}{\partial w_{mn}} = \eta \sum\limits_{\mu}
	(t_{m}^{(\mu)} - O_{m}^{(\mu)})V_{n}^{(\mu)}	
\end{gather}






One of the general issues one encounters when using gradient methods is the risk of getting stuck in a local minimum instead of the global minimum.




\subsection{Optimization algorithms}
\subsubsection{Gradient descent}
\subsubsection{AdaDelta}
\subsubsection{AdaGrad}
\subsubsection{RMSprop}
\subsubsection{Adam}




